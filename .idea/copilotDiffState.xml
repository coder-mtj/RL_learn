<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="updatedContent" value="# 强化学习算法实现项目&#10;&#10;本项目包含了多个经典强化学习算法的PyTorch实现，从简单的多臂赌博机到复杂的深度强化学习算法。每个实现都包含详细的注释和说明，适合学习和研究使用。&#10;&#10;## 项目结构&#10;&#10;- `demo_01_multi_arm_bandit.py`: 多臂赌博机算法实现&#10;- `demo_02_MDP.py`: 马尔可夫决策过程(MDP)的基本概念实现&#10;- `demo_03_DP.py`: 动态规划方法实现&#10;- `demo_04_env.py`: 强化学习环境实现&#10;- `demo_05_dyna_Q.py`: Dyna-Q算法实现&#10;- `demo_06_DQN.py`: 深度Q网络(DQN)算法实现&#10;- `demo_07_double_DQN.py`: Double DQN算法实现&#10;- `demo_08_dueling_DQN.py`: Dueling DQN算法实现&#10;- `rl_utils.py`: 强化学习工具函数库&#10;&#10;## 环境要求&#10;&#10;- Python 3.6+&#10;- PyTorch 1.0+&#10;- NumPy&#10;- Gym&#10;- Matplotlib&#10;- tqdm&#10;&#10;可以使用以下命令安装所需依赖：&#10;```bash&#10;pip install torch numpy gym matplotlib tqdm&#10;```&#10;&#10;## 算法说明&#10;&#10;### 1. 多臂赌博机 (Multi-Armed Bandit)&#10;实现了探索与利用的平衡策略，包括：&#10;- ε-贪婪策略&#10;- UCB算法&#10;- Thompson采样&#10;&#10;### 2. 马尔可夫决策过程 (MDP)&#10;包含MDP的基本要素实现：&#10;- 状态转移&#10;- 奖励函数&#10;- 价值函数&#10;&#10;### 3. 动态规划 (Dynamic Programming)&#10;实现了基于动态规划的算法：&#10;- 策略迭代&#10;- 价值迭代&#10;&#10;### 4. Dyna-Q&#10;实现了基于模型的强化学习算法Dyna-Q，包括：&#10;- Q学习更新&#10;- 模型学习&#10;- 规划更新&#10;&#10;### 5. DQN (Deep Q-Network)&#10;实现了DQN算法的核心组件：&#10;- 经验回放&#10;- 目标网络&#10;- ε-贪婪探索&#10;&#10;### 6. Double DQN&#10;在DQN的基础上改进，解决过度估计问题：&#10;- 分离动作选择和评估&#10;- 更稳定的Q值估计&#10;&#10;### 7. Dueling DQN&#10;实现了基于价值分解的DQN改进：&#10;- 状态价值流&#10;- 动作优势流&#10;- 优势函数归一化&#10;&#10;## 使用方法&#10;&#10;每个文件都可以独立运行，例如运行Dueling DQN：&#10;```bash&#10;python demo_08_dueling_DQN.py&#10;```&#10;&#10;## 关键特性&#10;&#10;1. 代码结构清晰，注释详细&#10;2. 实现了完整的训练和评估流程&#10;3. 包含可视化工具，方便分析算法性能&#10;4. 使用PyTorch实现，支持GPU加速&#10;&#10;## 实验结果&#10;&#10;各算法在典型环境（如CartPole-v0、Pendulum-v1）上都能达到良好的性能：&#10;- DQN和其变体能在CartPole-v0环境中实现稳定控制&#10;- Double DQN能有效缓解Q值过估计问题&#10;- Dueling DQN在某些任务中能获得更好的学习效率&#10;&#10;## 贡献指南&#10;&#10;欢迎提交Issue和Pull Request来改进代码和文档。在提交代码时请确保：&#10;1. 代码风格符合PEP 8规范&#10;2. 添加适当的注释和文档&#10;3. 确保代码可以正常运行&#10;&#10;## 参考资料&#10;&#10;- Sutton &amp; Barto的《强化学习：简介》&#10;- Deep Q-Network相关论文&#10;- OpenAI Gym文档&#10;&#10;## 许可证&#10;&#10;本项目采用MIT许可证。" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/demo_08_dueling_DQN.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/demo_08_dueling_DQN.py" />
              <option name="originalContent" value="# 导入所需的库&#10;import torch  # 导入PyTorch深度学习框架&#10;import torch.nn.functional as F  # 导入PyTorch中的函数性工具&#10;import numpy as np  # 导入数值计算库&#10;import random  # 导入随机数生成库&#10;import matplotlib.pyplot as plt  # 导入绘图库&#10;import gym  # 导入OpenAI Gym强化学习环境&#10;import rl_utils  # 导入自定义的强化学习工具库&#10;&#10;&#10;class Qnet(torch.nn.Module):&#10;    &quot;&quot;&quot;标准Q网络类，实现了一个简单的前馈神经网络。&#10;&#10;    这个网络包含一个隐藏层，用于近似Q值函数。&#10;&#10;    Args:&#10;        state_dim (int): 输入的状态维度&#10;        hidden_dim (int): 隐藏层的维度&#10;        action_dim (int): 输出的动作维度&#10;&#10;    Attributes:&#10;        fc1 (torch.nn.Linear): 第一个全连接层，从状态到隐藏层&#10;        fc2 (torch.nn.Linear): 第二个全连接层，从隐藏层到动作值&#10;    &quot;&quot;&quot;&#10;    def __init__(self, state_dim, hidden_dim, action_dim):&#10;        super(Qnet, self).__init__()&#10;        # 第一层全连接层，将状态映射到隐藏层&#10;        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)&#10;        # 第二层全连接层，将隐藏层映射到动作值&#10;        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;前向传播函数。&#10;&#10;        Args:&#10;            x (torch.Tensor): 输入状态张量&#10;&#10;        Returns:&#10;            torch.Tensor: 每个动作的Q值&#10;        &quot;&quot;&quot;&#10;        # 使用ReLU激活函数处理第一层的输出&#10;        x = F.relu(self.fc1(x))&#10;        # 返回最终的Q值预测&#10;        return self.fc2(x)&#10;&#10;&#10;class VAnet(torch.nn.Module):&#10;    &quot;&quot;&quot;Dueling DQN的价值优势网络类。&#10;&#10;    实现了具有价值流(V)和优势流(A)的神经网络架构。&#10;&#10;    Args:&#10;        state_dim (int): 输入的状态维度&#10;        hidden_dim (int): 隐藏层的维度&#10;        action_dim (int): 输出的动作维度&#10;&#10;    Attributes:&#10;        fc1 (torch.nn.Linear): 共享的特征提取层&#10;        fc_A (torch.nn.Linear): 优势流的全连接层&#10;        fc_V (torch.nn.Linear): 价值流的全连接层&#10;    &quot;&quot;&quot;&#10;    def __init__(self, state_dim, hidden_dim, action_dim):&#10;        super(VAnet, self).__init__()&#10;        # 共享特征提取层&#10;        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)&#10;        # 优势函数网络&#10;        self.fc_A = torch.nn.Linear(hidden_dim, action_dim)&#10;        # 价值函数网络&#10;        self.fc_V = torch.nn.Linear(hidden_dim, 1)&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;前向传播函数。&#10;&#10;        Args:&#10;            x (torch.Tensor): 输入状态张量&#10;&#10;        Returns:&#10;            torch.Tensor: 通过价值和优势计算得到的Q值&#10;        &quot;&quot;&quot;&#10;        # 提取共享特征并计算优势值&#10;        A = self.fc_A(F.relu(self.fc1(x)))&#10;        # 提取共享特征并计算状态值&#10;        V = self.fc_V(F.relu(self.fc1(x)))&#10;        # 结合优势和价值，减去平均优势以确保可识别性&#10;        Q = V + A - A.mean(1).view(-1, 1)&#10;        return Q&#10;&#10;&#10;class DQN:&#10;    &quot;&quot;&quot;DQN算法的实现类，支持vanilla DQN、Double DQN和Dueling DQN。&#10;&#10;    该类实现了DQN算法的核心功能，包括经验回放、目标网络更新等机制。&#10;&#10;    Args:&#10;        state_dim (int): 状态空间的维度&#10;        hidden_dim (int): 神经网络隐藏层的维度&#10;        action_dim (int): 动作空间的维度&#10;        learning_rate (float): 学习率&#10;        gamma (float): 折扣因子&#10;        epsilon (float): ε-贪婪策略中的探索率&#10;        target_update (int): 目标网络更新频率&#10;        device (str): 运行设备（'cpu'或'cuda'）&#10;        dqn_type (str, optional): DQN的类型，可选'VanillaDQN'、'DoubleDQN'或'DuelingDQN'&#10;&#10;    Attributes:&#10;        action_dim (int): 动作空间维度&#10;        q_net (torch.nn.Module): 主Q网络&#10;        target_q_net (torch.nn.Module): 目标Q网络&#10;        optimizer (torch.optim.Adam): 优化器&#10;        gamma (float): 折扣因子&#10;        epsilon (float): 探索率&#10;        target_update (int): 目标网络更新频率&#10;        count (int): 更新步数计数器&#10;        dqn_type (str): DQN类型&#10;        device (str): 运行设备&#10;    &quot;&quot;&quot;&#10;    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,&#10;                 epsilon, target_update, device, dqn_type='VanillaDQN'):&#10;        self.action_dim = action_dim&#10;        # 根据DQN类型选择不同的网络架构&#10;        if dqn_type == 'DuelingDQN':&#10;            # 使用价值优势网络作为Q网络和目标网络&#10;            self.q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;            self.target_q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;        else:&#10;            # 使用标准Q网络作为Q网络和目标网络&#10;            self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;            self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;&#10;        # 初始化Adam优化器&#10;        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)&#10;        # 设置其他超参数&#10;        self.gamma = gamma  # 折扣因子&#10;        self.epsilon = epsilon  # 探索率&#10;        self.target_update = target_update  # 目标网络更新频率&#10;        self.count = 0  # 更新次数计数器&#10;        self.dqn_type = dqn_type  # DQN类型&#10;        self.device = device  # 运行设备&#10;&#10;    def take_action(self, state):&#10;        &quot;&quot;&quot;选择动作的函数，实现ε-贪婪策略。&#10;&#10;        Args:&#10;            state (numpy.ndarray): 当前状态&#10;&#10;        Returns:&#10;            int: 选择的动作索引&#10;        &quot;&quot;&quot;&#10;        # epsilon贪婪策略：随机探索或选择最优动作&#10;        if np.random.random() &lt; self.epsilon:&#10;            # 随机选择动作&#10;            action = np.random.randint(self.action_dim)&#10;        else:&#10;            # 选择Q值最大的动作&#10;            state = torch.tensor([state], dtype=torch.float).to(self.device)&#10;            action = self.q_net(state).argmax().item()&#10;        return action&#10;&#10;    def max_q_value(self, state):&#10;        &quot;&quot;&quot;计算给定状态下的最大Q值。&#10;&#10;        Args:&#10;            state (numpy.ndarray): 输入状态&#10;&#10;        Returns:&#10;            float: 最大Q值&#10;        &quot;&quot;&quot;&#10;        # 将状态转换为张量并移到指定设备&#10;        state = torch.tensor([state], dtype=torch.float).to(self.device)&#10;        # 返回最大Q值&#10;        return self.q_net(state).max().item()&#10;&#10;    def update(self, transition_dict):&#10;        &quot;&quot;&quot;更新网络参数。&#10;&#10;        使用经验回放中的样本更新Q网络参数。&#10;&#10;        Args:&#10;            transition_dict (dict): 包含转换样本的字典，包括：&#10;                - 'states': 状态batch&#10;                - 'actions': 动作batch&#10;                - 'rewards': 奖励batch&#10;                - 'next_states': 下一状态batch&#10;                - 'dones': 终止标志batch&#10;        &quot;&quot;&quot;&#10;        # 将所有数据转换为张量并移到指定设备&#10;        states = torch.tensor(transition_dict['states'],&#10;                            dtype=torch.float).to(self.device)&#10;        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)&#10;        rewards = torch.tensor(transition_dict['rewards'],&#10;                             dtype=torch.float).view(-1, 1).to(self.device)&#10;        next_states = torch.tensor(transition_dict['next_states'],&#10;                                 dtype=torch.float).to(self.device)&#10;        dones = torch.tensor(transition_dict['dones'],&#10;                           dtype=torch.float).view(-1, 1).to(self.device)&#10;&#10;        # 计算当前Q值&#10;        q_values = self.q_net(states).gather(1, actions)&#10;&#10;        # 根据不同的DQN类型计算目标Q值&#10;        if self.dqn_type == 'DoubleDQN':&#10;            # Double DQN: 使用当前网络选择动作，目标网络评估动作&#10;            max_action = self.q_net(next_states).max(1)[1].view(-1, 1)&#10;            max_next_q_values = self.target_q_net(next_states).gather(1, max_action)&#10;        else:&#10;            # Vanilla DQN: 直接使用目标网络的最大Q值&#10;            max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1)&#10;&#10;        # 计算目标Q值&#10;        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)&#10;        # 计算均方误差损失&#10;        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))&#10;&#10;        # 优化模型参数&#10;        self.optimizer.zero_grad()  # 清除梯度&#10;        dqn_loss.backward()  # 反向传播&#10;        self.optimizer.step()  # 更新参数&#10;&#10;        # 定期更新目标网络&#10;        if self.count % self.target_update == 0:&#10;            self.target_q_net.load_state_dict(self.q_net.state_dict())&#10;        self.count += 1&#10;&#10;&#10;def train_DQN(agent, env, num_episodes, replay_buffer, minimal_size, batch_size):&#10;    &quot;&quot;&quot;训练DQN智能体的函数。&#10;&#10;    Args:&#10;        agent (DQN): DQN智能体实例&#10;        env (gym.Env): OpenAI Gym环境&#10;        num_episodes (int): 训练的总回合数&#10;        replay_buffer (ReplayBuffer): 经验回放缓冲区&#10;        minimal_size (int): 开始训练的最小样本数&#10;        batch_size (int): 每次训练的批量大小&#10;&#10;    Returns:&#10;        tuple: (return_list, max_q_value_list)&#10;            - return_list: 每个回合的回报列表&#10;            - max_q_value_list: 每个回合的最大Q值列表&#10;    &quot;&quot;&quot;&#10;    return_list = []  # 记录每个回合的回报&#10;    max_q_value_list = []  # 记录每个回合的最大Q值&#10;&#10;    for i_episode in range(num_episodes):&#10;        episode_return = 0  # 记录当前回合的累积回报&#10;        state, _ = env.reset()  # 重置环境，获取初始状态&#10;        max_q_value = 0  # 记录当前回合的最大Q值&#10;&#10;        while True:&#10;            # 根据当前状态选择动作&#10;            action = agent.take_action(state)&#10;            max_q_value = max(max_q_value, agent.max_q_value(state))  # 更新最大Q值&#10;            # 执行动作，获得下一状态和奖励&#10;            next_state, reward, terminated, truncated, _ = env.step(action)&#10;            done = terminated or truncated  # 合并两种终止情况&#10;            # 存储转移数据到经验回放缓冲区&#10;            replay_buffer.add(state, action, reward, next_state, done)&#10;            # 累积回报&#10;            episode_return += reward&#10;&#10;            # 如果样本数量达到要求，就进行训练&#10;            if replay_buffer.size() &gt; minimal_size:&#10;                # 从经验回放中采样数据进行训练&#10;                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)&#10;                transition_dict = {&#10;                    'states': b_s,&#10;                    'actions': b_a,&#10;                    'rewards': b_r,&#10;                    'next_states': b_ns,&#10;                    'dones': b_d&#10;                }&#10;                agent.update(transition_dict)  # 更新智能体&#10;&#10;            state = next_state  # 更新状态&#10;            if done:  # 如果回合结束，跳出循环&#10;                break&#10;&#10;        return_list.append(episode_return)  # 记录回合回报&#10;        max_q_value_list.append(max_q_value)  # 记录最大Q值&#10;&#10;        # 打印训练进度&#10;        print(f'Episode {i_episode+1}: Return = {episode_return:.2f}, Max Q-Value = {max_q_value:.2f}')&#10;&#10;    return return_list, max_q_value_list&#10;&#10;# 在主代码之前设置环境和超参数&#10;env_name = 'CartPole-v0'  # 定义环境名称&#10;env = gym.make(env_name)  # 创建环境实例&#10;state_dim = env.observation_space.shape[0]  # 状态空间维度&#10;hidden_dim = 128  # 隐藏层维度&#10;action_dim = env.action_space.n  # 动作空间维度&#10;lr = 2e-3  # 学习率&#10;gamma = 0.98  # 折扣因子&#10;epsilon = 0.1  # 探索率&#10;target_update = 10  # 目标网络更新频率&#10;buffer_size = 10000  # 经验回放池大小&#10;minimal_size = 500  # 开始训练的最小样本数&#10;batch_size = 64  # 批量大小&#10;num_episodes = 200  # 训练回合数&#10;device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)  # 设备选择&#10;&#10;# 设置随机种子以确保可重复性&#10;random.seed(0)  # 设置Python随机数生成器的种子&#10;np.random.seed(0)  # 设置NumPy随机数生成器的种子&#10;torch.manual_seed(0)  # 设置PyTorch随机数生成器的种子&#10;# 使用新版本的gym环境随机种子设置方式&#10;env = gym.make(env_name, render_mode=None)  # 重新创建环境&#10;env.action_space.seed(0)  # 设置动作空间的随机种子&#10;env.observation_space.seed(0)  # 设置观察空间的随机种子&#10;env.reset(seed=0)  # 设置环境的随机种子&#10;&#10;# 创建经验回放缓冲区&#10;replay_buffer = rl_utils.ReplayBuffer(buffer_size)&#10;&#10;# 创建Dueling DQN智能体&#10;agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,&#10;            target_update, device, 'DuelingDQN')&#10;&#10;# 训练智能体并获取训练数据&#10;return_list, max_q_value_list = train_DQN(agent, env, num_episodes,&#10;                                         replay_buffer, minimal_size,&#10;                                         batch_size)&#10;&#10;# 绘制训练回报曲线&#10;episodes_list = list(range(len(return_list)))  # 创建回合数列表&#10;mv_return = rl_utils.moving_average(return_list, 5)  # 计算移动平均回报&#10;plt.plot(episodes_list, mv_return)  # 绘制回报曲线&#10;plt.xlabel('Episodes')  # 设置x轴标签&#10;plt.ylabel('Returns')  # 设置y轴标签&#10;plt.title('Dueling DQN on {}'.format(env_name))  # 设置图标标题&#10;plt.show()  # 显示图表&#10;&#10;# 绘制Q值变化曲线&#10;frames_list = list(range(len(max_q_value_list)))  # 创建帧数列表&#10;plt.plot(frames_list, max_q_value_list)  # 绘制Q值曲线&#10;plt.axhline(0, c='orange', ls='--')  # 添加y=0参考线&#10;plt.axhline(10, c='red', ls='--')  # 添加y=10参考线&#10;plt.xlabel('Frames')  # 设置x轴标签&#10;plt.ylabel('Q value')  # 设置y轴标签&#10;plt.title('Dueling DQN on {}'.format(env_name))  # 设置图表标题&#10;plt.show()  # 显示图表&#10;" />
              <option name="updatedContent" value="# 导入所需的库&#10;import torch  # 导入PyTorch深度学习框架&#10;import torch.nn.functional as F  # 导入PyTorch中的函数性工具&#10;import numpy as np  # 导入数值计算库&#10;import random  # 导入随机数生成库&#10;import matplotlib.pyplot as plt  # 导入绘图库&#10;import gym  # 导入OpenAI Gym强化学习环境&#10;import rl_utils  # 导入自定义的强化学习工具库&#10;from tqdm import tqdm  # 添加tqdm用于显示进度条&#10;&#10;&#10;class Qnet(torch.nn.Module):&#10;    &quot;&quot;&quot;标准Q网络类，实现了一个简单的前馈神经网络。&#10;&#10;    这个网络包含一个隐藏层，用于近似Q值函数。&#10;&#10;    Args:&#10;        state_dim (int): 输入的状态维度&#10;        hidden_dim (int): 隐藏层的维度&#10;        action_dim (int): 输出的动作维度&#10;&#10;    Attributes:&#10;        fc1 (torch.nn.Linear): 第一个全连接层，从状态到隐藏层&#10;        fc2 (torch.nn.Linear): 第二个全连接层，从隐藏层到动作值&#10;    &quot;&quot;&quot;&#10;    def __init__(self, state_dim, hidden_dim, action_dim):&#10;        super(Qnet, self).__init__()&#10;        # 第一层全连接层，将状态映射到隐藏层&#10;        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)&#10;        # 第二层全连接层，将隐藏层映射到动作值&#10;        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;前向传播函数。&#10;&#10;        Args:&#10;            x (torch.Tensor): 输入状态张量&#10;&#10;        Returns:&#10;            torch.Tensor: 每个动作的Q值&#10;        &quot;&quot;&quot;&#10;        # 使用ReLU激活函数处理第一层的输出&#10;        x = F.relu(self.fc1(x))&#10;        # 返回最终的Q值预测&#10;        return self.fc2(x)&#10;&#10;&#10;class VAnet(torch.nn.Module):&#10;    &quot;&quot;&quot;Dueling DQN的价值优势网络类。&#10;&#10;    实现了具有价值流(V)和优势流(A)的神经网络架构。&#10;&#10;    Args:&#10;        state_dim (int): 输入的状态维度&#10;        hidden_dim (int): 隐藏层的维度&#10;        action_dim (int): 输出的动作维度&#10;&#10;    Attributes:&#10;        fc1 (torch.nn.Linear): 共享的特征提取层&#10;        fc_A (torch.nn.Linear): 优势流的全连接层&#10;        fc_V (torch.nn.Linear): 价值流的全连接层&#10;    &quot;&quot;&quot;&#10;    def __init__(self, state_dim, hidden_dim, action_dim):&#10;        super(VAnet, self).__init__()&#10;        # 共享特征提取层&#10;        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)&#10;        # 优势函数网络&#10;        self.fc_A = torch.nn.Linear(hidden_dim, action_dim)&#10;        # 价值函数网络&#10;        self.fc_V = torch.nn.Linear(hidden_dim, 1)&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;前向传播函数。&#10;&#10;        Args:&#10;            x (torch.Tensor): 输入状态张量&#10;&#10;        Returns:&#10;            torch.Tensor: 通过价值和优势计算得到的Q值&#10;        &quot;&quot;&quot;&#10;        # 提取共享特征并计算优势值&#10;        A = self.fc_A(F.relu(self.fc1(x)))&#10;        # 提取共享特征并计算状态值&#10;        V = self.fc_V(F.relu(self.fc1(x)))&#10;        # 结合优势和价值，减去平均优势以确保可识别性&#10;        Q = V + A - A.mean(1).view(-1, 1)&#10;        return Q&#10;&#10;&#10;class DQN:&#10;    &quot;&quot;&quot;DQN算法的实现类，支持vanilla DQN、Double DQN和Dueling DQN。&#10;&#10;    该类实现了DQN算法的核心功能，包括经验回放、目标网络更新等机制。&#10;&#10;    Args:&#10;        state_dim (int): 状态空间的维度&#10;        hidden_dim (int): 神经网络隐藏层的维度&#10;        action_dim (int): 动作空间的维度&#10;        learning_rate (float): 学习率&#10;        gamma (float): 折扣因子&#10;        epsilon (float): ε-贪婪策略中的探索率&#10;        target_update (int): 目标网络更新频率&#10;        device (str): 运行设备（'cpu'或'cuda'）&#10;        dqn_type (str, optional): DQN的类型，可选'VanillaDQN'、'DoubleDQN'或'DuelingDQN'&#10;&#10;    Attributes:&#10;        action_dim (int): 动作空间维度&#10;        q_net (torch.nn.Module): 主Q网络&#10;        target_q_net (torch.nn.Module): 目标Q网络&#10;        optimizer (torch.optim.Adam): 优化器&#10;        gamma (float): 折扣因子&#10;        epsilon (float): 探索率&#10;        target_update (int): 目标网络更新频率&#10;        count (int): 更新步数计数器&#10;        dqn_type (str): DQN类型&#10;        device (str): 运行设备&#10;    &quot;&quot;&quot;&#10;    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,&#10;                 epsilon, target_update, device, dqn_type='VanillaDQN'):&#10;        self.action_dim = action_dim&#10;        # 根据DQN类型选择不同的网络架构&#10;        if dqn_type == 'DuelingDQN':&#10;            # 使用价值优势网络作为Q网络和目标网络&#10;            self.q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;            self.target_q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;        else:&#10;            # 使用标准Q网络作为Q网络和目标网络&#10;            self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;            self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device)&#10;&#10;        # 初始化Adam优化器&#10;        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)&#10;        # 设置其他超参数&#10;        self.gamma = gamma  # 折扣因子&#10;        self.epsilon = epsilon  # 探索率&#10;        self.target_update = target_update  # 目标网络更新频率&#10;        self.count = 0  # 更新次数计数器&#10;        self.dqn_type = dqn_type  # DQN类型&#10;        self.device = device  # 运行设备&#10;&#10;    def take_action(self, state):&#10;        &quot;&quot;&quot;选择动作的函数，实现ε-贪婪策略。&#10;&#10;        Args:&#10;            state (numpy.ndarray): 当前状态&#10;&#10;        Returns:&#10;            int: 选择的动作索引&#10;        &quot;&quot;&quot;&#10;        # epsilon贪婪策略：随机探索或选择最优动作&#10;        if np.random.random() &lt; self.epsilon:&#10;            # 随机选择动作&#10;            action = np.random.randint(self.action_dim)&#10;        else:&#10;            # 选择Q值最大的动作&#10;            state = torch.tensor([state], dtype=torch.float).to(self.device)&#10;            action = self.q_net(state).argmax().item()&#10;        return action&#10;&#10;    def max_q_value(self, state):&#10;        &quot;&quot;&quot;计算给定状态下的最大Q值。&#10;&#10;        Args:&#10;            state (numpy.ndarray): 输入状态&#10;&#10;        Returns:&#10;            float: 最大Q值&#10;        &quot;&quot;&quot;&#10;        # 将状态转换为张量并移到指定设备&#10;        state = torch.tensor([state], dtype=torch.float).to(self.device)&#10;        # 返回最大Q值&#10;        return self.q_net(state).max().item()&#10;&#10;    def update(self, transition_dict):&#10;        &quot;&quot;&quot;更新网络参数。&#10;&#10;        使用经验回放中的样本更新Q网络参数。&#10;&#10;        Args:&#10;            transition_dict (dict): 包含转换样本的字典，包括：&#10;                - 'states': 状态batch&#10;                - 'actions': 动作batch&#10;                - 'rewards': 奖励batch&#10;                - 'next_states': 下一状态batch&#10;                - 'dones': 终止标志batch&#10;        &quot;&quot;&quot;&#10;        # 将所有数据转换为张量并移到指定设备&#10;        states = torch.tensor(transition_dict['states'],&#10;                            dtype=torch.float).to(self.device)&#10;        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)&#10;        rewards = torch.tensor(transition_dict['rewards'],&#10;                             dtype=torch.float).view(-1, 1).to(self.device)&#10;        next_states = torch.tensor(transition_dict['next_states'],&#10;                                 dtype=torch.float).to(self.device)&#10;        dones = torch.tensor(transition_dict['dones'],&#10;                           dtype=torch.float).view(-1, 1).to(self.device)&#10;&#10;        # 计算当前Q值&#10;        q_values = self.q_net(states).gather(1, actions)&#10;&#10;        # 根据不同的DQN类型计算目标Q值&#10;        if self.dqn_type == 'DoubleDQN':&#10;            # Double DQN: 使用当前网络选择动作，目标网络评估动作&#10;            max_action = self.q_net(next_states).max(1)[1].view(-1, 1)&#10;            max_next_q_values = self.target_q_net(next_states).gather(1, max_action)&#10;        else:&#10;            # Vanilla DQN: 直接使用目标网络的最大Q值&#10;            max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1)&#10;&#10;        # 计算目标Q值&#10;        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)&#10;        # 计算均方误差损失&#10;        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))&#10;&#10;        # 优化模型参数&#10;        self.optimizer.zero_grad()  # 清除梯度&#10;        dqn_loss.backward()  # 反向传播&#10;        self.optimizer.step()  # 更新参数&#10;&#10;        # 定期更新目标网络&#10;        if self.count % self.target_update == 0:&#10;            self.target_q_net.load_state_dict(self.q_net.state_dict())&#10;        self.count += 1&#10;&#10;&#10;def train_DQN(agent, env, num_episodes, replay_buffer, minimal_size, batch_size):&#10;    &quot;&quot;&quot;训练DQN智能体的函数。&#10;&#10;    Args:&#10;        agent (DQN): DQN智能体实例&#10;        env (gym.Env): OpenAI Gym环境&#10;        num_episodes (int): 训练的总回合数&#10;        replay_buffer (ReplayBuffer): 经验回放缓冲区&#10;        minimal_size (int): 开始训练的最小样本数&#10;        batch_size (int): 每次训练的批量大小&#10;&#10;    Returns:&#10;        tuple: (return_list, max_q_value_list)&#10;            - return_list: 每个回合的回报列表&#10;            - max_q_value_list: 每个回合的最大Q值列表&#10;    &quot;&quot;&quot;&#10;    return_list = []  # 记录每个回合的回报&#10;    max_q_value_list = []  # 记录每个回合的最大Q值&#10;    max_q_value = 0  # 初始化最大Q值&#10;&#10;    # 分10次迭代训练&#10;    for i in range(10):&#10;        with tqdm(total=int(num_episodes / 10),&#10;                  desc='Iteration %d' % i) as pbar:&#10;            # 每次迭代训练一部分回合&#10;            for i_episode in range(int(num_episodes / 10)):&#10;                episode_return = 0  # 记录当前回合的累积回报&#10;                state, _ = env.reset()  # 重置环境，获取初始状态&#10;                done = False&#10;                &#10;                # 进行一回合训练&#10;                while not done:&#10;                    action = agent.take_action(state)  # 选择动作&#10;                    # 平滑处理最大Q值&#10;                    max_q_value = agent.max_q_value(state) * 0.005 + max_q_value * 0.995&#10;                    max_q_value_list.append(max_q_value)  # 保存当前状态的最大Q值&#10;                    &#10;                    # 执行动作，获得下一状态和奖励&#10;                    next_state, reward, terminated, truncated, _ = env.step(action)&#10;                    done = terminated or truncated  # 合并两种终止情况&#10;                    &#10;                    # 存储转移数据到经验回放缓冲区&#10;                    replay_buffer.add(state, action, reward, next_state, done)&#10;                    state = next_state&#10;                    episode_return += reward  # 累积回报&#10;                    &#10;                    # 如果样本数量达到要求，就进行训练&#10;                    if replay_buffer.size() &gt; minimal_size:&#10;                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)&#10;                        transition_dict = {&#10;                            'states': b_s,&#10;                            'actions': b_a,&#10;                            'rewards': b_r,&#10;                            'next_states': b_ns,&#10;                            'dones': b_d&#10;                        }&#10;                        agent.update(transition_dict)  # 更新智能体&#10;                &#10;                return_list.append(episode_return)  # 记录回合回报&#10;                &#10;                # 每训练10个回合，更新进度条&#10;                if (i_episode + 1) % 10 == 0:&#10;                    pbar.set_postfix({&#10;                        'episode': &#10;                        '%d' % (num_episodes / 10 * i + i_episode + 1),&#10;                        'return': &#10;                        '%.3f' % np.mean(return_list[-10:])&#10;                    })&#10;                pbar.update(1)  # 更新进度条&#10;    &#10;    return return_list, max_q_value_list&#10;&#10;# 在主代码之前设置环境和超参数&#10;env_name = 'CartPole-v0'  # 定义环境名称&#10;env = gym.make(env_name)  # 创建环境实例&#10;state_dim = env.observation_space.shape[0]  # 状态空间维度&#10;hidden_dim = 128  # 隐藏层维度&#10;action_dim = env.action_space.n  # 动作空间维度&#10;lr = 2e-3  # 学习率&#10;gamma = 0.98  # 折扣因子&#10;epsilon = 0.1  # 探索率&#10;target_update = 10  # 目标网络更新频率&#10;buffer_size = 10000  # 经验回放池大小&#10;minimal_size = 500  # 开始训练的最小样本数&#10;batch_size = 64  # 批量大小&#10;num_episodes = 200  # 训练回合数&#10;device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)  # 设备选择&#10;&#10;# 设置随机种子以确保可重复性&#10;random.seed(0)  # 设置Python随机数生成器的种子&#10;np.random.seed(0)  # 设置NumPy随机数生成器的种子&#10;torch.manual_seed(0)  # 设置PyTorch随机数生成器的种子&#10;# 使用新版本的gym环境随机种子设置方式&#10;env = gym.make(env_name, render_mode=None)  # 重新创建环境&#10;env.action_space.seed(0)  # 设置动作空间的随机种子&#10;env.observation_space.seed(0)  # 设置观察空间的随机种子&#10;env.reset(seed=0)  # 设置环境的随机种子&#10;&#10;# 创建经验回放缓冲区&#10;replay_buffer = rl_utils.ReplayBuffer(buffer_size)&#10;&#10;# 创建Dueling DQN智能体&#10;agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,&#10;            target_update, device, 'DuelingDQN')&#10;&#10;# 训练智能体并获取训练数据&#10;return_list, max_q_value_list = train_DQN(agent, env, num_episodes,&#10;                                         replay_buffer, minimal_size,&#10;                                         batch_size)&#10;&#10;# 绘制训练回报曲线&#10;episodes_list = list(range(len(return_list)))  # 创建回合数列表&#10;mv_return = rl_utils.moving_average(return_list, 5)  # 计算移动平均回报&#10;plt.plot(episodes_list, mv_return)  # 绘制回报曲线&#10;plt.xlabel('Episodes')  # 设置x轴标签&#10;plt.ylabel('Returns')  # 设置y轴标签&#10;plt.title('Dueling DQN on {}'.format(env_name))  # 设置图标标题&#10;plt.show()  # 显示图表&#10;&#10;# 绘制Q值变化曲线&#10;frames_list = list(range(len(max_q_value_list)))  # 创建帧数列表&#10;plt.plot(frames_list, max_q_value_list)  # 绘制Q值曲线&#10;plt.axhline(0, c='orange', ls='--')  # 添加y=0参考线&#10;plt.axhline(10, c='red', ls='--')  # 添加y=10参考线&#10;plt.xlabel('Frames')  # 设置x轴标签&#10;plt.ylabel('Q value')  # 设置y轴标签&#10;plt.title('Dueling DQN on {}'.format(env_name))  # 设置图表标题&#10;plt.show()  # 显示图表" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/demo_08_shape_example.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/demo_08_shape_example.py" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/rl_utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/rl_utils.py" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>