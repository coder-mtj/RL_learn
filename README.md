# å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°é›†åˆ

æœ¬é¡¹ç›®å®ç°äº†ä¸€ç³»åˆ—ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»åŸºç¡€çš„å¤šè‡‚èµŒåšæœºåˆ°é«˜çº§çš„TRPOç®—æ³•ï¼Œæä¾›äº†å®Œæ•´çš„ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚

## é¡¹ç›®ç»“æ„

### åŸºç¡€ç®—æ³•
- `demo_01_multi_arm_bandit.py`: å¤šè‡‚èµŒåšæœºé—®é¢˜å®ç°
- `demo_02_MDP.py`: é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åŸºç¡€å®ç°
- `demo_03_DP.py`: åŠ¨æ€è§„åˆ’æ–¹æ³•
- `demo_04_env.py`: å¼ºåŒ–å­¦ä¹ ç¯å¢ƒå®ç°

### è¿›é˜¶ç®—æ³•
- `demo_05_dyna_Q.py`: Dyna-Qç®—æ³•å®ç°
- `demo_06_DQN.py`: æ·±åº¦Qç½‘ç»œåŸºç¡€å®ç°
- `demo_07_double_DQN.py`: åŒé‡DQNç®—æ³•
- `demo_08_dueling_DQN.py`: å†³æ–—DQNç½‘ç»œç»“æ„

### é«˜çº§ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- `demo_09_REINFORCE.py`: REINFORCEç®—æ³•ï¼ˆåŸºç¡€ç­–ç•¥æ¢¯åº¦ï¼‰
- `demo_10_Actor_Critic.py`: Actor-Criticæ¶æ„å®ç°
- `demo_11_TRPO_CartPole.py`: TRPOç®—æ³•åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆCartPoleç¯å¢ƒï¼‰
- `demo_12_TRPO_pendulum.py`: TRPOç®—æ³•åœ¨è¿ç»­åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆPendulumç¯å¢ƒï¼‰

## TRPOç®—æ³•è¯¦ç»†è¯´æ˜

TRPOï¼ˆTrust Region Policy Optimizationï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦æ¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œå•è°ƒæ”¹è¿›ã€‚æœ¬é¡¹ç›®æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å®ç°ï¼š

### ğŸ“‹ å®ç°ç‰ˆæœ¬å¯¹æ¯”

| ç‰¹æ€§ | demo_11_TRPO_CartPole.py | demo_12_TRPO_pendulum.py |
|------|-------------------------|--------------------------|
| **åŠ¨ä½œç©ºé—´** | ç¦»æ•£åŠ¨ä½œï¼ˆCategoricalåˆ†å¸ƒï¼‰ | è¿ç»­åŠ¨ä½œï¼ˆNormalåˆ†å¸ƒï¼‰ |
| **ç¯å¢ƒ** | CartPole-v1 | Pendulum-v1 |
| **ç­–ç•¥ç½‘ç»œè¾“å‡º** | åŠ¨ä½œæ¦‚ç‡å‘é‡ | é«˜æ–¯åˆ†å¸ƒå‚æ•°ï¼ˆÎ¼, Ïƒï¼‰ |
| **åŠ¨ä½œé‡‡æ ·** | ç±»åˆ«åˆ†å¸ƒé‡‡æ · | æ­£æ€åˆ†å¸ƒé‡‡æ · |
| **KLæ•£åº¦è®¡ç®—** | ç¦»æ•£åˆ†å¸ƒKLæ•£åº¦ | è¿ç»­åˆ†å¸ƒKLæ•£åº¦ |
| **é€‚ç”¨åœºæ™¯** | æ¸¸æˆã€å¯¼èˆªç­‰ç¦»æ•£å†³ç­– | æœºå™¨äººæ§åˆ¶ã€è¿ç»­æ§åˆ¶ |

### 1. æ•°å­¦åŸç†

TRPOç®—æ³•çš„æ ¸å¿ƒæ˜¯æ±‚è§£å¦‚ä¸‹çº¦æŸä¼˜åŒ–é—®é¢˜ï¼š

```
æœ€å¤§åŒ–ï¼šE[Ï€_new(a|s)/Ï€_old(a|s) * A(s,a)]
çº¦æŸæ¡ä»¶ï¼šE[KL(Ï€_old(Â·|s) || Ï€_new(Â·|s))] â‰¤ Î´
```

å…¶ä¸­ï¼š
- Ï€_new å’Œ Ï€_old åˆ†åˆ«æ˜¯æ–°æ—§ç­–ç•¥
- A(s,a) æ˜¯ä¼˜åŠ¿å‡½æ•°
- KL æ˜¯KLæ•£åº¦ï¼Œç”¨äºåº¦é‡ç­–ç•¥æ›´æ–°çš„å¹…åº¦
- Î´ æ˜¯KLæ•£åº¦çº¦æŸé˜ˆå€¼

#### 1.1 è¿ç»­åŠ¨ä½œç©ºé—´çš„ç‰¹æ®Šå¤„ç†

å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œç­–ç•¥ç½‘ç»œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ï¼š
```
Ï€(a|s) = N(Î¼(s), Ïƒ(s))
```
å…¶ä¸­Î¼(s)å’ŒÏƒ(s)åˆ†åˆ«æ˜¯çŠ¶æ€sä¸‹çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚

### 2. å…³é”®ç»„ä»¶å®ç°

#### 2.1 å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
```python
def compute_advantage(gamma, lmbda, td_delta):
    """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡
    A(s,a) = Î´_t + (Î³Î»)Î´_{t+1} + ... + (Î³Î»)^{T-t-1}Î´_{T-1}
    å…¶ä¸­ Î´_t æ˜¯TDè¯¯å·®
    """
    advantage = 0.0
    advantage_list = []
    for delta in td_delta[::-1]:  # ä»åå‘å‰è®¡ç®—
        advantage = gamma * lmbda * advantage + delta
        advantage_list.append(advantage)
    advantage_list.reverse()
    return torch.tensor(advantage_list, dtype=torch.float)
```

#### 2.2 ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰

**ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆCartPoleï¼‰ï¼š**
```python
class PolicyNet(torch.nn.Module):
    """å°†çŠ¶æ€æ˜ å°„ä¸ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒçš„ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)  # è¾“å‡ºåŠ¨ä½œæ¦‚ç‡
```

**è¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆPendulumï¼‰ï¼š**
```python
class PolicyNetContinuous(torch.nn.Module):
    """è¿ç»­åŠ¨ä½œç©ºé—´çš„ç­–ç•¥ç½‘ç»œï¼Œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒå‚æ•°"""
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNetContinuous, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)    # å‡å€¼
        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)   # æ ‡å‡†å·®

    def forward(self, x):
        x = F.relu(self.fc1(x))
        mu = 2.0 * torch.tanh(self.fc_mu(x))    # é™åˆ¶åŠ¨ä½œèŒƒå›´
        std = F.softplus(self.fc_std(x))        # ç¡®ä¿æ ‡å‡†å·®ä¸ºæ­£
        return mu, std
```

#### 2.3 ä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰
```python
class ValueNet(torch.nn.Module):
    """è¯„ä¼°çŠ¶æ€ä»·å€¼çš„ç½‘ç»œï¼ˆä¸¤ä¸ªç‰ˆæœ¬é€šç”¨ï¼‰"""
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

### 3. ç®—æ³•æ ¸å¿ƒæ­¥éª¤

TRPOç®—æ³•çš„æ‰§è¡Œæµç¨‹åŒ…å«ä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š

1. **æ”¶é›†è½¨è¿¹æ•°æ®**
   - ä½¿ç”¨å½“å‰ç­–ç•¥Ï€_oldä¸ç¯å¢ƒäº¤äº’
   - å­˜å‚¨çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€çŠ¶æ€ç­‰ä¿¡æ¯
   - æ”¶é›†å®Œæ•´çš„episodeæ•°æ®

2. **è®¡ç®—ä¼˜åŠ¿ä¼°è®¡**
   - ä½¿ç”¨GAEæ–¹æ³•è®¡ç®—ä¼˜åŠ¿å‡½æ•°å€¼A(s,a)
   - ç»“åˆæ—¶åºå·®åˆ†è¯¯å·®å’Œå¤šæ­¥å›æŠ¥
   - å¹³è¡¡åå·®å’Œæ–¹å·®

3. **ç­–ç•¥æ›´æ–°ï¼ˆTRPOæ ¸å¿ƒï¼‰**
   - **æ­¥éª¤3.1**: è®¡ç®—æ›¿ä»£ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦g
   - **æ­¥éª¤3.2**: ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•æ±‚è§£Hx = gï¼Œå¾—åˆ°è‡ªç„¶æ¢¯åº¦æ–¹å‘x
   - **æ­¥éª¤3.3**: è®¡ç®—æœ€å¤§æ­¥é•¿ï¼šmax_coef = âˆš(2Î´/(x^T H x))
   - **æ­¥éª¤3.4**: æ‰§è¡Œçº¿æ€§æœç´¢ï¼Œç¡®ä¿æ»¡è¶³KLçº¦æŸå’Œæ€§èƒ½æ”¹è¿›
   - **æ­¥éª¤3.5**: æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°

4. **ä»·å€¼å‡½æ•°æ›´æ–°**
   - ä½¿ç”¨æ—¶åºå·®åˆ†ç›®æ ‡æ›´æ–°ä»·å€¼ç½‘ç»œ
   - æœ€å°åŒ–ä»·å€¼ä¼°è®¡çš„å‡æ–¹è¯¯å·®
   - ä¸ºä¸‹ä¸€è½®ä¼˜åŠ¿ä¼°è®¡æä¾›åŸºå‡†

#### 3.1 å…±è½­æ¢¯åº¦æ³•è¯¦è§£

å…±è½­æ¢¯åº¦æ³•ç”¨äºé«˜æ•ˆæ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„Hx = gï¼š
```python
def conjugate_gradient(self, grad, states, old_action_dists):
    x = torch.zeros_like(grad)  # åˆå§‹è§£
    r = grad.clone()            # åˆå§‹æ®‹å·®
    p = grad.clone()            # åˆå§‹æœç´¢æ–¹å‘

    for i in range(10):  # æœ€å¤š10æ¬¡è¿­ä»£
        Hp = self.hessian_matrix_vector_product(states, old_action_dists, p)
        alpha = torch.dot(r, r) / torch.dot(p, Hp)
        x += alpha * p
        r -= alpha * Hp

        if torch.dot(r, r) < 1e-10:  # æ”¶æ•›æ£€æŸ¥
            break

        beta = torch.dot(r, r) / torch.dot(r_old, r_old)
        p = r + beta * p

    return x
```

#### 3.2 çº¿æ€§æœç´¢æœºåˆ¶

çº¿æ€§æœç´¢ç¡®ä¿ç­–ç•¥æ›´æ–°æ—¢æ”¹å–„æ€§èƒ½åˆæ»¡è¶³çº¦æŸï¼š
```python
def line_search(self, states, actions, advantage, old_log_probs,
                old_action_dists, max_vec):
    for i in range(15):  # æœ€å¤š15æ¬¡å°è¯•
        coef = self.alpha ** i  # æ­¥é•¿è¡°å‡
        new_para = old_para + coef * max_vec

        # æ£€æŸ¥çº¦æŸæ¡ä»¶
        if new_obj > old_obj and kl_div < self.kl_constraint:
            return new_para  # æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„æ›´æ–°

    return old_para  # ä¿å®ˆç­–ç•¥ï¼šä¸æ›´æ–°
```

### 4. å‚æ•°é…ç½®

#### 4.1 CartPoleç¯å¢ƒï¼ˆç¦»æ•£åŠ¨ä½œï¼‰
```python
# ç½‘ç»œå‚æ•°
hidden_dim = 128        # éšè—å±‚ç»´åº¦

# è®­ç»ƒå‚æ•°
num_episodes = 500      # è®­ç»ƒå›åˆæ•°
gamma = 0.98           # æŠ˜æ‰£å› å­
lmbda = 0.95          # GAEå‚æ•°
critic_lr = 1e-2       # è¯„è®ºå®¶å­¦ä¹ ç‡

# TRPOç‰¹æœ‰å‚æ•°
kl_constraint = 0.0005  # KLæ•£åº¦çº¦æŸ
alpha = 0.5            # çº¿æ€§æœç´¢æ­¥é•¿è¡°å‡
```

#### 4.2 Pendulumç¯å¢ƒï¼ˆè¿ç»­åŠ¨ä½œï¼‰
```python
# ç½‘ç»œå‚æ•°
hidden_dim = 128        # éšè—å±‚ç»´åº¦

# è®­ç»ƒå‚æ•°
num_episodes = 2000     # è®­ç»ƒå›åˆæ•°ï¼ˆè¿ç»­æ§åˆ¶éœ€è¦æ›´å¤šè®­ç»ƒï¼‰
gamma = 0.9            # æŠ˜æ‰£å› å­
lmbda = 0.9           # GAEå‚æ•°
critic_lr = 1e-2       # è¯„è®ºå®¶å­¦ä¹ ç‡

# TRPOç‰¹æœ‰å‚æ•°
kl_constraint = 0.00005 # KLæ•£åº¦çº¦æŸï¼ˆæ›´ä¸¥æ ¼ï¼‰
alpha = 0.5            # çº¿æ€§æœç´¢æ­¥é•¿è¡°å‡
```

#### 4.3 å‚æ•°è°ƒä¼˜å»ºè®®

| å‚æ•° | ä½œç”¨ | è°ƒä¼˜å»ºè®® |
|------|------|----------|
| `kl_constraint` | æ§åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ | è¿‡å¤§â†’ä¸ç¨³å®šï¼›è¿‡å°â†’å­¦ä¹ æ…¢ |
| `lmbda` | GAEåå·®-æ–¹å·®æƒè¡¡ | æ¥è¿‘1â†’ä½åå·®é«˜æ–¹å·®ï¼›æ¥è¿‘0â†’é«˜åå·®ä½æ–¹å·® |
| `gamma` | æœªæ¥å¥–åŠ±é‡è¦æ€§ | é•¿æœŸä»»åŠ¡ç”¨0.99ï¼›çŸ­æœŸä»»åŠ¡ç”¨0.9 |
| `alpha` | çº¿æ€§æœç´¢æ¿€è¿›ç¨‹åº¦ | 0.5æ˜¯ç»éªŒå€¼ï¼Œå¯å°è¯•0.3-0.8 |

### 5. ä½¿ç”¨è¯´æ˜

#### 5.1 ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch numpy matplotlib tqdm

# å®‰è£…gymnasiumåº“
pip install gymnasium[classic_control]
```

#### 5.2 è¿è¡Œç¦»æ•£åŠ¨ä½œç‰ˆæœ¬ï¼ˆCartPoleï¼‰
```python
# è¿è¡Œdemo_11_TRPO_CartPole.py
python demo_11_TRPO_CartPole.py

# åˆ›å»ºæ™ºèƒ½ä½“ç¤ºä¾‹
env = gym.make('CartPole-v1')
agent = TRPO(hidden_dim, env.observation_space, env.action_space,
            lmbda, kl_constraint, alpha, critic_lr, gamma, device)
return_list = train_on_policy_agent(env, agent, num_episodes)
```

#### 5.3 è¿è¡Œè¿ç»­åŠ¨ä½œç‰ˆæœ¬ï¼ˆPendulumï¼‰
```python
# è¿è¡Œdemo_12_TRPO_pendulum.py
python demo_12_TRPO_pendulum.py

# åˆ›å»ºæ™ºèƒ½ä½“ç¤ºä¾‹
env = gym.make('Pendulum-v1')
agent = TRPOContinuous(hidden_dim, env.observation_space, env.action_space,
                      lmbda, kl_constraint, alpha, critic_lr, gamma, device)
return_list = train_on_policy_agent(env, agent, num_episodes)
```

#### 5.4 ç»“æœå¯è§†åŒ–
```python
# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
episodes_list = list(range(len(return_list)))

# åŸå§‹æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(episodes_list, return_list)
plt.title('Raw Training Progress')

# ç§»åŠ¨å¹³å‡æ›²çº¿
plt.subplot(1, 2, 2)
mv_return = moving_average(return_list, 9)
plt.plot(episodes_list, mv_return)
plt.title('Smoothed Training Progress')

plt.show()
```

### 6. ä¼˜åŠ¿å’Œç‰¹ç‚¹

#### 6.1 ç®—æ³•ä¼˜åŠ¿
1. **ç¨³å®šæ€§**ï¼šé€šè¿‡KLæ•£åº¦çº¦æŸç¡®ä¿ç­–ç•¥æ›´æ–°çš„ä¿å®ˆæ€§ï¼Œé¿å…ç­–ç•¥å´©æºƒ
2. **å¯é æ€§**ï¼šç†è®ºä¸Šä¿è¯å•è°ƒç­–ç•¥æ”¹è¿›ï¼Œæ¯æ¬¡æ›´æ–°éƒ½ä¸ä¼šå˜å·®
3. **é€‚ç”¨æ€§**ï¼šåŒæ—¶æ”¯æŒç¦»æ•£å’Œè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œåº”ç”¨èŒƒå›´å¹¿æ³›
4. **æ•ˆç‡**ï¼šä½¿ç”¨å…±è½­æ¢¯åº¦æ³•é«˜æ•ˆæ±‚è§£äºŒé˜¶ä¼˜åŒ–é—®é¢˜ï¼Œé¿å…ç›´æ¥è®¡ç®—HessiançŸ©é˜µ

#### 6.2 å®ç°ç‰¹ç‚¹
1. **å®Œæ•´æ³¨é‡Š**ï¼šæ¯è¡Œå…³é”®ä»£ç éƒ½æœ‰è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
2. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„ç±»ç»“æ„ï¼Œä¾¿äºç†è§£å’Œä¿®æ”¹
3. **å…¼å®¹æ€§å¥½**ï¼šæ”¯æŒæ–°æ—§ç‰ˆæœ¬çš„gym/gymnasium
4. **å¯è§†åŒ–ä¸°å¯Œ**ï¼šæä¾›å¤šç§è®­ç»ƒæ›²çº¿å’Œæ€§èƒ½åˆ†æå›¾è¡¨

#### 6.3 æ€§èƒ½è¡¨ç°

**CartPoleç¯å¢ƒï¼ˆç¦»æ•£åŠ¨ä½œï¼‰ï¼š**
- é€šå¸¸åœ¨100-200ä¸ªepisodeså†…è¾¾åˆ°æ»¡åˆ†ï¼ˆ500åˆ†ï¼‰
- è®­ç»ƒç¨³å®šï¼Œå¾ˆå°‘å‡ºç°æ€§èƒ½å€’é€€
- é€‚åˆä½œä¸ºTRPOç®—æ³•çš„å…¥é—¨ç¤ºä¾‹

**Pendulumç¯å¢ƒï¼ˆè¿ç»­åŠ¨ä½œï¼‰ï¼š**
- ä»åˆå§‹-1400å·¦å³é€æ­¥æ”¹å–„åˆ°-200ä»¥å†…
- éœ€è¦æ›´å¤šè®­ç»ƒepisodesï¼ˆ2000+ï¼‰
- å±•ç¤ºäº†TRPOåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­çš„èƒ½åŠ›

### 7. æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ

#### 7.1 å…³é”®æ³¨æ„äº‹é¡¹
1. **KLæ•£åº¦çº¦æŸ**ï¼šè¿™æ˜¯æœ€å…³é”®çš„è¶…å‚æ•°
   - è¿‡å¤§ï¼ˆ>0.01ï¼‰ï¼šå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œç­–ç•¥æ›´æ–°è¿‡äºæ¿€è¿›
   - è¿‡å°ï¼ˆ<0.0001ï¼‰ï¼šå­¦ä¹ é€Ÿåº¦æ…¢ï¼Œç­–ç•¥æ›´æ–°è¿‡äºä¿å®ˆ
   - æ¨èå€¼ï¼šç¦»æ•£åŠ¨ä½œ0.0005ï¼Œè¿ç»­åŠ¨ä½œ0.00005

2. **æ‰¹é‡æ•°æ®å¤§å°**ï¼šéœ€è¦è¶³å¤Ÿçš„æ•°æ®æ¥å‡†ç¡®ä¼°è®¡KLæ•£åº¦
   - æ¯ä¸ªepisodeçš„æ•°æ®éƒ½å¾ˆé‡è¦
   - ä¸å»ºè®®ä½¿ç”¨mini-batchï¼Œè€Œæ˜¯ä½¿ç”¨å®Œæ•´episodeæ•°æ®

3. **è®¡ç®—å¤æ‚åº¦**ï¼šç›¸æ¯”ç®€å•çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•æ›´å¤æ‚
   - å…±è½­æ¢¯åº¦æ³•éœ€è¦å¤šæ¬¡Hessian-vectorä¹˜ç§¯è®¡ç®—
   - çº¿æ€§æœç´¢éœ€è¦å¤šæ¬¡å‰å‘ä¼ æ’­
   - å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿ

4. **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šå¯¹å‚æ•°è®¾ç½®è¾ƒä¸ºæ•æ„Ÿ
   - å»ºè®®ä»æ¨èå€¼å¼€å§‹ï¼Œé€æ­¥å¾®è°ƒ
   - ä¸åŒç¯å¢ƒå¯èƒ½éœ€è¦ä¸åŒçš„å‚æ•°è®¾ç½®

#### 7.2 è°ƒè¯•å’Œä¼˜åŒ–å»ºè®®

**è®­ç»ƒä¸ç¨³å®šçš„è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. å‡å°KLæ•£åº¦çº¦æŸ
kl_constraint = 0.0001  # ä»0.0005å‡å°åˆ°0.0001

# 2. è°ƒæ•´GAEå‚æ•°
lmbda = 0.8  # ä»0.95å‡å°åˆ°0.8ï¼Œé™ä½æ–¹å·®

# 3. å¢åŠ é˜»å°¼ç³»æ•°
damping = 0.1  # åœ¨hessian_matrix_vector_productä¸­
```

**è®­ç»ƒé€Ÿåº¦æ…¢çš„è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. å‡å°ç½‘ç»œè§„æ¨¡
hidden_dim = 64  # ä»128å‡å°åˆ°64

# 2. å‡å°‘å…±è½­æ¢¯åº¦è¿­ä»£æ¬¡æ•°
cg_iters = 5  # ä»10å‡å°åˆ°5

# 3. å‡å°‘çº¿æ€§æœç´¢æ¬¡æ•°
max_backtracks = 10  # ä»15å‡å°åˆ°10
```

**æ€§èƒ½ä¸ç†æƒ³çš„è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. å¢åŠ è®­ç»ƒepisodes
num_episodes = 1000  # é€‚å½“å¢åŠ 

# 2. è°ƒæ•´å¥–åŠ±ç¼©æ”¾ï¼ˆç‰¹åˆ«æ˜¯Pendulumï¼‰
rewards = (rewards + 8.0) / 8.0  # å¥–åŠ±æ ‡å‡†åŒ–

# 3. è°ƒæ•´ç½‘ç»œåˆå§‹åŒ–
torch.nn.init.orthogonal_(layer.weight, gain=0.01)
```

### 8. å®éªŒç»“æœå±•ç¤º

#### 8.1 CartPoleç¯å¢ƒè®­ç»ƒç»“æœ
```
ç¯å¢ƒ: CartPole-v1
è®­ç»ƒepisodes: 500
æœ€ç»ˆæ€§èƒ½: 500.0 (æ»¡åˆ†)
æ”¶æ•›é€Ÿåº¦: ~150 episodes
ç¨³å®šæ€§: é«˜ï¼Œå¾ˆå°‘å‡ºç°æ€§èƒ½å€’é€€
```

#### 8.2 Pendulumç¯å¢ƒè®­ç»ƒç»“æœ
```
ç¯å¢ƒ: Pendulum-v1
è®­ç»ƒepisodes: 2000
åˆå§‹æ€§èƒ½: -1453.235
æœ€ç»ˆæ€§èƒ½: -474.927
æ”¹å–„å¹…åº¦: ~67%
æ”¶æ•›ç‰¹ç‚¹: é€æ­¥ç¨³å®šæ”¹å–„ï¼Œæ— æ˜æ˜¾éœ‡è¡
```

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### åŸºç¡€ä¾èµ–
- **Python**: 3.7+ ï¼ˆæ¨è3.8+ï¼‰
- **PyTorch**: 1.8+ ï¼ˆæ¨èæœ€æ–°ç‰ˆæœ¬ï¼‰
- **NumPy**: 1.19+
- **Matplotlib**: 3.3+
- **tqdm**: 4.60+ ï¼ˆè¿›åº¦æ¡æ˜¾ç¤ºï¼‰

### å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
```bash
# ç»Ÿä¸€ä½¿ç”¨gymnasiumåº“
pip install gymnasium[classic_control]
```

### å¯é€‰åŠ é€Ÿ
- **CUDA**: æ”¯æŒGPUåŠ é€Ÿï¼ˆæ¨èï¼‰
- **cuDNN**: æ·±åº¦å­¦ä¹ åŠ é€Ÿåº“

### å®Œæ•´å®‰è£…å‘½ä»¤
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n rl_env python=3.8
conda activate rl_env

# å®‰è£…PyTorchï¼ˆæ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
pip install torch torchvision torchaudio

# å®‰è£…å…¶ä»–ä¾èµ–
pip install gymnasium[classic_control] numpy matplotlib tqdm

# éªŒè¯å®‰è£…
python -c "import torch; import gymnasium; print('å®‰è£…æˆåŠŸï¼')"
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **TRPOåŸå§‹è®ºæ–‡**: Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. In International conference on machine learning (pp. 1889-1897).

2. **GAEè®ºæ–‡**: Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.

3. **ç­–ç•¥æ¢¯åº¦ç»¼è¿°**: Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12.

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### è´¡çŒ®æ–¹å¼
1. Forkæœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

### ä»£ç è§„èŒƒ
- ä¿æŒè¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
- éµå¾ªGoogle Pythonä»£ç é£æ ¼
- æ·»åŠ é€‚å½“çš„ç±»å‹æç¤º
- ç¡®ä¿ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ

---

**é¡¹ç›®ç»´æŠ¤è€…**: [POPO]
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ
**è®¸å¯è¯**: MIT License
