# å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°é›†åˆ

æœ¬é¡¹ç›®å®ç°äº†ä¸€ç³»åˆ—ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»åŸºç¡€çš„å¤šè‡‚èµŒåšæœºåˆ°é«˜çº§çš„TRPOç®—æ³•ï¼Œæä¾›äº†å®Œæ•´çš„ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚

## é¡¹ç›®ç»“æ„

### åŸºç¡€ç®—æ³•
- `demo_01_multi_arm_bandit.py`: å¤šè‡‚èµŒåšæœºé—®é¢˜å®ç°
- `demo_02_MDP.py`: é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åŸºç¡€å®ç°
- `demo_03_DP.py`: åŠ¨æ€è§„åˆ’æ–¹æ³•
- `demo_04_env.py`: å¼ºåŒ–å­¦ä¹ ç¯å¢ƒå®ç°

### è¿›é˜¶ç®—æ³•
- `demo_05_dyna_Q.py`: Dyna-Qç®—æ³•å®ç°
- `demo_06_DQN.py`: æ·±åº¦Qç½‘ç»œåŸºç¡€å®ç°
- `demo_07_double_DQN.py`: åŒé‡DQNç®—æ³•
- `demo_08_dueling_DQN.py`: å†³æ–—DQNç½‘ç»œç»“æ„

### é«˜çº§ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- `demo_09_REINFORCE.py`: REINFORCEç®—æ³•ï¼ˆåŸºç¡€ç­–ç•¥æ¢¯åº¦ï¼‰
- `demo_10_Actor_Critic.py`: Actor-Criticæ¶æ„å®ç°
- `demo_11_TRPO_CartPole.py`: TRPOç®—æ³•åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆCartPoleç¯å¢ƒï¼‰
- `demo_12_TRPO_pendulum.py`: TRPOç®—æ³•åœ¨è¿ç»­åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆPendulumç¯å¢ƒï¼‰
- `demo_13_PPO.py`: PPOç®—æ³•åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆCartPoleç¯å¢ƒï¼‰
- `demo_14_PPO_Pendulum.py`: PPOç®—æ³•åœ¨è¿ç»­åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆPendulumç¯å¢ƒï¼‰

## ç®—æ³•è¯´æ˜

### PPOç®—æ³•
PPOï¼ˆProximal Policy Optimizationï¼‰æ˜¯ç›®å‰æœ€æµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œæœ¬é¡¹ç›®æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å®ç°ï¼š

- **demo_13_PPO.py**: ç¦»æ•£åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆCartPoleç¯å¢ƒï¼‰
- **demo_14_PPO_Pendulum.py**: è¿ç»­åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆPendulumç¯å¢ƒï¼‰

### TRPOç®—æ³•
TRPOï¼ˆTrust Region Policy Optimizationï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œæœ¬é¡¹ç›®æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å®ç°ï¼š

- **demo_11_TRPO_CartPole.py**: ç¦»æ•£åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆCartPoleç¯å¢ƒï¼‰
- **demo_12_TRPO_pendulum.py**: è¿ç»­åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆPendulumç¯å¢ƒï¼‰

## ä½¿ç”¨è¯´æ˜

### ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch numpy matplotlib tqdm

# å®‰è£…gymnasiumåº“
pip install gymnasium[classic_control]
```

### è¿è¡Œç®—æ³•

**PPOç®—æ³•**ï¼š
```bash
# ç¦»æ•£åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆCartPoleç¯å¢ƒï¼‰
python demo_13_PPO.py

# è¿ç»­åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆPendulumç¯å¢ƒï¼‰
python demo_14_PPO_Pendulum.py
```

**TRPOç®—æ³•**ï¼š
```bash
# ç¦»æ•£åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆCartPoleç¯å¢ƒï¼‰
python demo_11_TRPO_CartPole.py

# è¿ç»­åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆPendulumç¯å¢ƒï¼‰
python demo_12_TRPO_pendulum.py
```

**å…¶ä»–ç®—æ³•**ï¼š
```bash
# REINFORCEç®—æ³•
python demo_09_REINFORCE.py

# Actor-Criticç®—æ³•
python demo_10_Actor_Critic.py

# DQNç³»åˆ—ç®—æ³•
python demo_06_DQN.py
python demo_07_double_DQN.py
python demo_08_dueling_DQN.py
```

### æ€§èƒ½è¡¨ç°

**CartPole-v1ç¯å¢ƒï¼ˆç¦»æ•£åŠ¨ä½œï¼‰**ï¼š
- æ”¶æ•›é€Ÿåº¦ï¼šé€šå¸¸åœ¨100-200ä¸ªepisodeså†…è¾¾åˆ°æ»¡åˆ†ï¼ˆ500åˆ†ï¼‰
- è®­ç»ƒç¨³å®šæ€§ï¼šè®­ç»ƒè¿‡ç¨‹å¹³ç¨³ï¼Œå¾ˆå°‘å‡ºç°æ€§èƒ½å¤§å¹…æ³¢åŠ¨
- æœ€ç»ˆæ€§èƒ½ï¼šèƒ½å¤Ÿç¨³å®šè¾¾åˆ°æœ€å¤§å¥–åŠ±500åˆ†

**Pendulum-v1ç¯å¢ƒï¼ˆè¿ç»­åŠ¨ä½œï¼‰**ï¼š
- åˆå§‹æ€§èƒ½ï¼šçº¦-1171.6ï¼ˆç¬¬50å›åˆï¼‰
- æœ€ä½³æ€§èƒ½ï¼šçº¦-625.3ï¼ˆç¬¬450å›åˆï¼‰
- æ”¹å–„å¹…åº¦ï¼šçº¦47%çš„æ€§èƒ½æå‡
- è®­ç»ƒç‰¹ç‚¹ï¼šæ•´ä½“å‘ˆç°æ”¹å–„è¶‹åŠ¿ï¼Œä½†å­˜åœ¨ä¸€å®šæ³¢åŠ¨

### æ³¨æ„äº‹é¡¹

#### ç¯å¢ƒä¸ç®—æ³•åŒ¹é…
- **ç¦»æ•£åŠ¨ä½œç©ºé—´**ï¼šä½¿ç”¨ `demo_13_PPO.py` æˆ– `demo_11_TRPO_CartPole.py`
- **è¿ç»­åŠ¨ä½œç©ºé—´**ï¼šä½¿ç”¨ `demo_14_PPO_Pendulum.py` æˆ– `demo_12_TRPO_pendulum.py`
- **åŠ¨ä½œç»´åº¦è·å–**ï¼š
  - ç¦»æ•£ï¼š`env.action_space.n`
  - è¿ç»­ï¼š`env.action_space.shape[0]`

#### å¸¸è§é”™è¯¯
```python
# âŒ é”™è¯¯ï¼šè¿ç»­ç®—æ³• + ç¦»æ•£ç¯å¢ƒ
env = gym.make('CartPole-v1')  # ç¦»æ•£åŠ¨ä½œç©ºé—´
agent = PPOContinuous(...)     # è¿ç»­åŠ¨ä½œç®—æ³•

# âœ… æ­£ç¡®ï¼šåŒ¹é…çš„ç»„åˆ
env = gym.make('CartPole-v1')  # ç¦»æ•£åŠ¨ä½œç©ºé—´
agent = PPO(...)               # ç¦»æ•£åŠ¨ä½œç®—æ³•
```



## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **PPOåŸå§‹è®ºæ–‡**: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

2. **TRPOåŸå§‹è®ºæ–‡**: Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. In International conference on machine learning (pp. 1889-1897).

3. **GAEè®ºæ–‡**: Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.

4. **ç­–ç•¥æ¢¯åº¦ç»¼è¿°**: Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12.

5. **OpenAI PPOå®ç°**: Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., ... & Wu, J. (2017). OpenAI baselines. GitHub repository.

---

**é¡¹ç›®ç»´æŠ¤è€…**: [POPO]
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ
**è®¸å¯è¯**: MIT License
