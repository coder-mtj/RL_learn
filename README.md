# å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°é›†åˆ

æœ¬é¡¹ç›®å®ç°äº†ä¸€ç³»åˆ—ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»åŸºç¡€çš„å¤šè‡‚èµŒåšæœºåˆ°é«˜çº§çš„TRPOç®—æ³•ï¼Œæä¾›äº†å®Œæ•´çš„ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚

## é¡¹ç›®ç»“æ„

### åŸºç¡€ç®—æ³•
- `demo_01_multi_arm_bandit.py`: å¤šè‡‚èµŒåšæœºé—®é¢˜å®ç°
- `demo_02_MDP.py`: é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åŸºç¡€å®ç°
- `demo_03_DP.py`: åŠ¨æ€è§„åˆ’æ–¹æ³•
- `demo_04_env.py`: å¼ºåŒ–å­¦ä¹ ç¯å¢ƒå®ç°

### è¿›é˜¶ç®—æ³•
- `demo_05_dyna_Q.py`: Dyna-Qç®—æ³•å®ç°
- `demo_06_DQN.py`: æ·±åº¦Qç½‘ç»œåŸºç¡€å®ç°
- `demo_07_double_DQN.py`: åŒé‡DQNç®—æ³•
- `demo_08_dueling_DQN.py`: å†³æ–—DQNç½‘ç»œç»“æ„

### é«˜çº§ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- `demo_09_REINFORCE.py`: REINFORCEç®—æ³•ï¼ˆåŸºç¡€ç­–ç•¥æ¢¯åº¦ï¼‰
- `demo_10_Actor_Critic.py`: Actor-Criticæ¶æ„å®ç°
- `demo_11_TRPO_CartPole.py`: TRPOç®—æ³•åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆCartPoleç¯å¢ƒï¼‰
- `demo_12_TRPO_pendulum.py`: TRPOç®—æ³•åœ¨è¿ç»­åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆPendulumç¯å¢ƒï¼‰
- `demo_13_PPO_CartPole.py`: PPOç®—æ³•åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆCartPoleç¯å¢ƒï¼‰
- `demo_14_PPO_Pendulum.py`: PPOç®—æ³•åœ¨è¿ç»­åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆPendulumç¯å¢ƒï¼‰

### é«˜çº§Actor-Criticæ–¹æ³•
- `demo_16_SAC.py`: SACï¼ˆSoft Actor-Criticï¼‰ç®—æ³•åœ¨è¿ç»­åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆPendulumç¯å¢ƒï¼‰

### æ¨¡ä»¿å­¦ä¹ æ–¹æ³•
- `demo_17_behavior_cloning.py`: è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼‰ç®—æ³•å®ç°ï¼ŒåŒ…å«PPOä¸“å®¶è®­ç»ƒå’ŒBCæ¨¡ä»¿å­¦ä¹ å¯¹æ¯”

## ç®—æ³•è¯´æ˜

### PPOç®—æ³•
PPOï¼ˆProximal Policy Optimizationï¼‰æ˜¯ç›®å‰æœ€æµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œæœ¬é¡¹ç›®æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å®ç°ï¼š

- **demo_13_PPO_CartPole.py**: ç¦»æ•£åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆCartPoleç¯å¢ƒï¼‰
- **demo_14_PPO_Pendulum.py**: è¿ç»­åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆPendulumç¯å¢ƒï¼‰

**PPOæŸå¤±å‡½æ•°**ï¼š
```python
# æˆªæ–­ä»£ç†ç›®æ ‡å‡½æ•°ï¼ˆClipped Surrogate Objectiveï¼‰
ratio = torch.exp(log_probs - old_log_probs)  # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
surr1 = ratio * advantage                     # ç¬¬ä¸€ä¸ªä»£ç†ç›®æ ‡
surr2 = torch.clamp(ratio, 1-eps, 1+eps) * advantage  # æˆªæ–­ä»£ç†ç›®æ ‡
actor_loss = torch.mean(-torch.min(surr1, surr2))     # PPOç­–ç•¥æŸå¤±

# ä»·å€¼å‡½æ•°æŸå¤±
critic_loss = torch.mean(F.mse_loss(critic(states), td_target))
```

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- ä½¿ç”¨æˆªæ–­æœºåˆ¶é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
- åŒæ—¶ä¼˜åŒ–ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œ
- åŸºäºç­–ç•¥æ¢¯åº¦å’Œä¼˜åŠ¿å‡½æ•°

### TRPOç®—æ³•
TRPOï¼ˆTrust Region Policy Optimizationï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œæœ¬é¡¹ç›®æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å®ç°ï¼š

- **demo_11_TRPO_CartPole.py**: ç¦»æ•£åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆCartPoleç¯å¢ƒï¼‰
- **demo_12_TRPO_pendulum.py**: è¿ç»­åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆPendulumç¯å¢ƒï¼‰

### SACç®—æ³•
SACï¼ˆSoft Actor-Criticï¼‰æ˜¯ä¸€ç§åŸºäºæœ€å¤§ç†µçš„Actor-Criticç®—æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºè¿ç»­æ§åˆ¶ä»»åŠ¡ï¼š

- **demo_16_SAC.py**: è¿ç»­åŠ¨ä½œç©ºé—´ç‰ˆæœ¬ï¼ˆPendulumç¯å¢ƒï¼‰

**SACæŸå¤±å‡½æ•°**ï¼š
```python
# ç­–ç•¥æŸå¤±ï¼ˆæœ€å¤§ç†µç›®æ ‡ï¼‰
entropy = -log_prob.sum(dim=-1, keepdim=True)  # ç­–ç•¥ç†µ
q1_value = critic_1(states, actions)
q2_value = critic_2(states, actions)
actor_loss = torch.mean(-log_alpha.exp() * entropy - torch.min(q1_value, q2_value))

# Qç½‘ç»œæŸå¤±ï¼ˆåŒQå­¦ä¹ ï¼‰
q1_loss = F.mse_loss(q1_value, td_target.detach())
q2_loss = F.mse_loss(q2_value, td_target.detach())

# æ¸©åº¦å‚æ•°æŸå¤±ï¼ˆè‡ªåŠ¨è°ƒèŠ‚ï¼‰
alpha_loss = torch.mean(-log_alpha * (entropy + target_entropy).detach())
```

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- æœ€å¤§ç†µåŸç†ï¼šå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
- åŒQç½‘ç»œï¼šå‡å°‘ä»·å€¼è¿‡ä¼°è®¡
- è‡ªåŠ¨æ¸©åº¦è°ƒèŠ‚ï¼šåŠ¨æ€å¹³è¡¡ç†µä¸å¥–åŠ±
- æ ·æœ¬æ•ˆç‡é«˜ã€è®­ç»ƒç¨³å®šã€æ¢ç´¢èƒ½åŠ›å¼º

### è¡Œä¸ºå…‹éš†ç®—æ³•
è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼‰æ˜¯ä¸€ç§æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ æ¨¡ä»¿ä¸“å®¶è¡Œä¸ºï¼š

- **demo_17_behavior_cloning.py**: å®Œæ•´çš„BCå®ç°ï¼ŒåŒ…å«PPOä¸“å®¶è®­ç»ƒå’ŒBCæ¨¡ä»¿å­¦ä¹ 

**BCæŸå¤±å‡½æ•°**ï¼š
```python
# è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰
log_probs = torch.log(policy(states).gather(1, actions))  # è®¡ç®—å¯¹æ•°æ¦‚ç‡
bc_loss = torch.mean(-log_probs)  # è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±

# ç­‰ä»·äºäº¤å‰ç†µæŸå¤±
# bc_loss = F.cross_entropy(policy_logits, actions)
```

**æ•°å­¦åŸç†**ï¼š
```
ä¸“å®¶æ•°æ®ä¼¼ç„¶: L(Î¸) = âˆ P(a_i | s_i, Î¸)
å¯¹æ•°ä¼¼ç„¶: log L(Î¸) = âˆ‘ log P(a_i | s_i, Î¸)
æŸå¤±å‡½æ•°: Loss = -log L(Î¸) = -âˆ‘ log P(a_i | s_i, Î¸)
```

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- ç›‘ç£å­¦ä¹ èŒƒå¼ï¼šç›´æ¥å­¦ä¹ çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„
- æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼šæœ€å¤§åŒ–ä¸“å®¶åŠ¨ä½œçš„æ¦‚ç‡
- å•ç½‘ç»œç»“æ„ï¼šåªéœ€è¦ç­–ç•¥ç½‘ç»œï¼Œæ— éœ€ä»·å€¼ç½‘ç»œ
- ä¸“å®¶æ•°æ®ä¾èµ–ï¼šæ€§èƒ½å—é™äºä¸“å®¶æ•°æ®è´¨é‡å’Œæ•°é‡
- **åº”ç”¨åœºæ™¯**: è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAI

## ğŸ” æŸå¤±å‡½æ•°å¯¹æ¯”åˆ†æ

### ä¸‰ç§ç®—æ³•çš„æŸå¤±å‡½æ•°æœ¬è´¨åŒºåˆ«

| ç®—æ³• | æŸå¤±å‡½æ•°ç±»å‹ | æ•°å­¦åŸºç¡€ | ä¼˜åŒ–ç›®æ ‡ | ç½‘ç»œç»“æ„ |
|------|-------------|----------|----------|----------|
| **PPO** | æˆªæ–­ä»£ç†ç›®æ ‡ | ç­–ç•¥æ¢¯åº¦ + ä¿¡ä»»åŸŸ | æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ï¼ˆæœ‰çº¦æŸï¼‰ | Actor + Critic |
| **SAC** | æœ€å¤§ç†µç›®æ ‡ | ç­–ç•¥æ¢¯åº¦ + æœ€å¤§ç†µ | æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ + ç†µ | Actor + åŒCritic |
| **BC** | è´Ÿå¯¹æ•°ä¼¼ç„¶ | æœ€å¤§ä¼¼ç„¶ä¼°è®¡ | æœ€å¤§åŒ–ä¸“å®¶åŠ¨ä½œæ¦‚ç‡ | ä»…Policy |

### è¯¦ç»†å¯¹æ¯”

**1. PPO - å¼ºåŒ–å­¦ä¹ ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰**
```python
# æ ¸å¿ƒï¼šé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢æ€§èƒ½å´©å¡Œ
actor_loss = -torch.min(ratio * advantage, clipped_ratio * advantage)
```
- **ç‰¹ç‚¹**ï¼šé€šè¿‡æˆªæ–­æœºåˆ¶ç¡®ä¿ç­–ç•¥æ›´æ–°ç¨³å®š
- **é€‚ç”¨**ï¼šéœ€è¦ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ çš„ä»»åŠ¡
- **ä¼˜åŠ¿**ï¼šè®­ç»ƒç¨³å®šï¼Œç†è®ºä¿è¯

**2. SAC - å¼ºåŒ–å­¦ä¹ ï¼ˆæœ€å¤§ç†µï¼‰**
```python
# æ ¸å¿ƒï¼šå¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–å’Œç­–ç•¥ç†µæœ€å¤§åŒ–
actor_loss = -torch.min(q1, q2) - alpha * entropy
```
- **ç‰¹ç‚¹**ï¼šé¼“åŠ±æ¢ç´¢ï¼Œè‡ªåŠ¨è°ƒèŠ‚æ¢ç´¢-åˆ©ç”¨å¹³è¡¡
- **é€‚ç”¨**ï¼šè¿ç»­æ§åˆ¶ä»»åŠ¡ï¼Œéœ€è¦é«˜æ ·æœ¬æ•ˆç‡
- **ä¼˜åŠ¿**ï¼šæ ·æœ¬æ•ˆç‡é«˜ï¼Œæ¢ç´¢èƒ½åŠ›å¼º

**3. BC - ç›‘ç£å­¦ä¹ ï¼ˆæ¨¡ä»¿å­¦ä¹ ï¼‰**
```python
# æ ¸å¿ƒï¼šç›´æ¥æ¨¡ä»¿ä¸“å®¶è¡Œä¸ºï¼Œæœ€å¤§åŒ–ä¸“å®¶åŠ¨ä½œæ¦‚ç‡
bc_loss = -torch.mean(log_probs)  # ç­‰ä»·äºäº¤å‰ç†µ
```
- **ç‰¹ç‚¹**ï¼šæ— éœ€ç¯å¢ƒäº¤äº’ï¼Œç›´æ¥ä»ä¸“å®¶æ•°æ®å­¦ä¹ 
- **é€‚ç”¨**ï¼šæœ‰é«˜è´¨é‡ä¸“å®¶æ¼”ç¤ºçš„ä»»åŠ¡
- **ä¼˜åŠ¿**ï¼šè®­ç»ƒç®€å•ï¼Œæ— éœ€å¥–åŠ±å‡½æ•°

### æŸå¤±å‡½æ•°çš„æ•°å­¦å«ä¹‰

**PPOæŸå¤±å‡½æ•°**ï¼š
- ç›®æ ‡ï¼š`max E[min(r(Î¸)A, clip(r(Î¸))A)]`
- å«ä¹‰ï¼šåœ¨ä¿¡ä»»åŸŸå†…æœ€å¤§åŒ–ä¼˜åŠ¿åŠ æƒçš„ç­–ç•¥æ”¹è¿›

**SACæŸå¤±å‡½æ•°**ï¼š
- ç›®æ ‡ï¼š`max E[R + Î±Â·H(Ï€)]`
- å«ä¹‰ï¼šåŒæ—¶æœ€å¤§åŒ–å¥–åŠ±å’Œç­–ç•¥ç†µï¼ˆæ¢ç´¢æ€§ï¼‰

**BCæŸå¤±å‡½æ•°**ï¼š
- ç›®æ ‡ï¼š`max âˆP(a_expert|s)`
- å«ä¹‰ï¼šæœ€å¤§åŒ–åœ¨ä¸“å®¶çŠ¶æ€ä¸‹é€‰æ‹©ä¸“å®¶åŠ¨ä½œçš„æ¦‚ç‡

## ä½¿ç”¨è¯´æ˜

### ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch numpy matplotlib tqdm

# å®‰è£…gymnasiumåº“
pip install gymnasium[classic_control]
```

#### å¸¸è§é”™è¯¯
```python
# âŒ é”™è¯¯ï¼šè¿ç»­ç®—æ³• + ç¦»æ•£ç¯å¢ƒ
env = gym.make('CartPole-v1')  # ç¦»æ•£åŠ¨ä½œç©ºé—´
agent = PPOContinuous(...)     # è¿ç»­åŠ¨ä½œç®—æ³•

# âœ… æ­£ç¡®ï¼šåŒ¹é…çš„ç»„åˆ
env = gym.make('CartPole-v1')  # ç¦»æ•£åŠ¨ä½œç©ºé—´
agent = PPO(...)               # ç¦»æ•£åŠ¨ä½œç®—æ³•

# âœ… è¡Œä¸ºå…‹éš†æ­£ç¡®ä½¿ç”¨
# 1. å…ˆè®­ç»ƒä¸“å®¶ï¼ˆPPOï¼‰
# 2. é‡‡æ ·ä¸“å®¶æ•°æ®
# 3. è®­ç»ƒBCæ™ºèƒ½ä½“
```



## ğŸ“š å‚è€ƒæ–‡çŒ®

### ç­–ç•¥æ¢¯åº¦æ–¹æ³•
1. **PPOåŸå§‹è®ºæ–‡**: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

2. **TRPOåŸå§‹è®ºæ–‡**: Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. In International conference on machine learning (pp. 1889-1897).

3. **GAEè®ºæ–‡**: Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.

### Actor-Criticæ–¹æ³•
4. **SACåŸå§‹è®ºæ–‡**: Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning (pp. 1861-1870).

5. **SACæ”¹è¿›ç‰ˆæœ¬**: Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... & Levine, S. (2018). Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.

### æ¨¡ä»¿å­¦ä¹ æ–¹æ³•
6. **è¡Œä¸ºå…‹éš†ç»¼è¿°**: Pomerleau, D. A. (1991). Efficient training of artificial neural networks for autonomous navigation. Neural computation, 3(1), 88-97.

7. **æ¨¡ä»¿å­¦ä¹ ç†è®º**: Ross, S., Gordon, G., & Bagnell, D. (2011). A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics (pp. 627-635).

### åŸºç¡€ç†è®º
8. **ç­–ç•¥æ¢¯åº¦ç»¼è¿°**: Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12.

9. **OpenAIå®ç°å‚è€ƒ**: Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., ... & Wu, J. (2017). OpenAI baselines. GitHub repository.

---

## ğŸš€ æœ€æ–°æ›´æ–°

### v2.1.0 (2025å¹´8æœˆ)
- âœ… æ–°å¢SACï¼ˆSoft Actor-Criticï¼‰ç®—æ³•å®ç°
- âœ… æ–°å¢è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼‰ç®—æ³•å®ç°
- âœ… å®Œå–„ä»£ç æ³¨é‡Šï¼Œæ¯è¡Œä»£ç éƒ½æœ‰è¯¦ç»†è¯´æ˜
- âœ… æ·»åŠ è®­ç»ƒè¿›åº¦æ¡å’Œæ€§èƒ½å¯è§†åŒ–
- âœ… ä¿®å¤gymnasiumå…¼å®¹æ€§é—®é¢˜

### v2.0.0 (2025å¹´7æœˆ)
- âœ… å‡çº§åˆ°gymnasiumåº“ï¼ˆæ›¿ä»£å·²å¼ƒç”¨çš„gymï¼‰
- âœ… å®Œå–„PPOå’ŒTRPOç®—æ³•å®ç°
- âœ… æ·»åŠ è¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´æ”¯æŒ
- âœ… ä¼˜åŒ–ä»£ç ç»“æ„å’Œæ³¨é‡Š

---

**é¡¹ç›®ç»´æŠ¤è€…**: [POPO]
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ
**è®¸å¯è¯**: MIT License

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·éšæ—¶è”ç³»ã€‚

### å¼€å‘è®¡åˆ’
- [ ] æ·»åŠ æ›´å¤šæ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼ˆGAILã€ValueDiceç­‰ï¼‰
- [ ] å®ç°å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•
- [ ] æ·»åŠ æ›´å¤šç¯å¢ƒæ”¯æŒ
- [ ] ä¼˜åŒ–ç®—æ³•æ€§èƒ½å’Œç¨³å®šæ€§
