# å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°é›†åˆ

æœ¬é¡¹ç›®å®ç°äº†ä¸€ç³»åˆ—ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»åŸºç¡€çš„å¤šè‡‚èµŒåšæœºåˆ°é«˜çº§çš„TRPOç®—æ³•ï¼Œæä¾›äº†å®Œæ•´çš„ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚

## é¡¹ç›®ç»“æ„

### åŸºç¡€ç®—æ³•
- `demo_01_multi_arm_bandit.py`: å¤šè‡‚èµŒåšæœºé—®é¢˜å®ç°
- `demo_02_MDP.py`: é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åŸºç¡€å®ç°
- `demo_03_DP.py`: åŠ¨æ€è§„åˆ’æ–¹æ³•
- `demo_04_env.py`: å¼ºåŒ–å­¦ä¹ ç¯å¢ƒå®ç°

### è¿›é˜¶ç®—æ³•
- `demo_05_dyna_Q.py`: Dyna-Qç®—æ³•å®ç°
- `demo_06_DQN.py`: æ·±åº¦Qç½‘ç»œåŸºç¡€å®ç°
- `demo_07_double_DQN.py`: åŒé‡DQNç®—æ³•
- `demo_08_dueling_DQN.py`: å†³æ–—DQNç½‘ç»œç»“æ„

### é«˜çº§ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- `demo_09_REINFORCE.py`: REINFORCEç®—æ³•ï¼ˆåŸºç¡€ç­–ç•¥æ¢¯åº¦ï¼‰
- `demo_10_Actor_Critic.py`: Actor-Criticæ¶æ„å®ç°
- `demo_11_TRPO_CartPole.py`: TRPOç®—æ³•åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆCartPoleç¯å¢ƒï¼‰
- `demo_12_TRPO_pendulum.py`: TRPOç®—æ³•åœ¨è¿ç»­åŠ¨ä½œç©ºé—´çš„å®ç°ï¼ˆPendulumç¯å¢ƒï¼‰
- `demo_13_PPO.py`: PPOç®—æ³•å®Œæ•´å®ç°ï¼ˆCartPoleç¯å¢ƒï¼‰

## PPOç®—æ³•è¯¦ç»†è¯´æ˜

PPOï¼ˆProximal Policy Optimizationï¼‰æ˜¯ç›®å‰æœ€æµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œå®ƒç®€åŒ–äº†TRPOçš„å¤æ‚å®ç°ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜ç§€çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„PPOå®ç°ï¼ŒåŒ…å«è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šã€‚

### ğŸ“‹ PPOç®—æ³•ç‰¹ç‚¹

| ç‰¹æ€§ | PPO | TRPO |
|------|-----|------|
| **å®ç°å¤æ‚åº¦** | ç®€å•ï¼Œæ˜“äºç†è§£å’Œå®ç° | å¤æ‚ï¼Œéœ€è¦å…±è½­æ¢¯åº¦æ³• |
| **è®¡ç®—æ•ˆç‡** | é«˜æ•ˆï¼Œåªéœ€ä¸€é˜¶æ¢¯åº¦ | è¾ƒæ…¢ï¼Œéœ€è¦äºŒé˜¶ä¿¡æ¯ |
| **ç¨³å®šæ€§** | é€šè¿‡æˆªæ–­ä¿è¯ç¨³å®šæ€§ | é€šè¿‡KLçº¦æŸä¿è¯ç¨³å®šæ€§ |
| **è¶…å‚æ•°æ•æ„Ÿæ€§** | ç›¸å¯¹é²æ£’ | å¯¹KLçº¦æŸæ•æ„Ÿ |
| **é€‚ç”¨åœºæ™¯** | å¹¿æ³›åº”ç”¨äºå„ç§RLä»»åŠ¡ | ç†è®ºä¿è¯æ›´å¼ºä½†å®ç°å¤æ‚ |

### 1. PPOæ•°å­¦åŸç†

PPOç®—æ³•çš„æ ¸å¿ƒæ˜¯æˆªæ–­çš„ä»£ç†ç›®æ ‡å‡½æ•°ï¼ˆClipped Surrogate Objectiveï¼‰ï¼š

```
L^CLIP(Î¸) = E_t[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]
```

å…¶ä¸­ï¼š
- `r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)` æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡
- `A_t` æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰
- `Îµ` æ˜¯æˆªæ–­å‚æ•°ï¼ˆé€šå¸¸è®¾ä¸º0.2ï¼‰
- `clip(x, a, b)` å°†xé™åˆ¶åœ¨[a, b]èŒƒå›´å†…

#### 1.1 ä¸ºä»€ä¹ˆè¦æˆªæ–­ï¼Ÿ

PPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯**ä¿å®ˆçš„ç­–ç•¥æ›´æ–°**ï¼š

1. **å½“ä¼˜åŠ¿ä¸ºæ­£æ—¶**ï¼ˆå¥½åŠ¨ä½œï¼‰ï¼š
   - å¦‚æœ `r_t(Î¸) > 1+Îµ`ï¼ˆæ–°ç­–ç•¥è¿‡äºæ¿€è¿›ï¼‰ï¼Œä½¿ç”¨æˆªæ–­å€¼ `1+Îµ`
   - é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ï¼Œé¿å…æ€§èƒ½å´©æºƒ

2. **å½“ä¼˜åŠ¿ä¸ºè´Ÿæ—¶**ï¼ˆååŠ¨ä½œï¼‰ï¼š
   - å¦‚æœ `r_t(Î¸) < 1-Îµ`ï¼ˆæ–°ç­–ç•¥è¿‡äºä¿å®ˆï¼‰ï¼Œä½¿ç”¨æˆªæ–­å€¼ `1-Îµ`
   - ç¡®ä¿å¯¹ååŠ¨ä½œçš„æƒ©ç½šä¸ä¼šè¿‡åº¦

#### 1.2 æŸå¤±å‡½æ•°å®ç°

åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†æœ€å¤§åŒ–é—®é¢˜è½¬æ¢ä¸ºæœ€å°åŒ–é—®é¢˜ï¼š

```python
# è®¡ç®—ä¸¤ä¸ªä»£ç†ç›®æ ‡
surr1 = ratio * advantage                    # æœªæˆªæ–­ç‰ˆæœ¬
surr2 = torch.clamp(ratio, 1-Îµ, 1+Îµ) * advantage  # æˆªæ–­ç‰ˆæœ¬

# å–æœ€å°å€¼ï¼ˆä¿å®ˆç­–ç•¥ï¼‰å¹¶è½¬æ¢ä¸ºæŸå¤±å‡½æ•°
actor_loss = -torch.mean(torch.min(surr1, surr2))
```

**ä¸ºä»€ä¹ˆå–è´Ÿæ•°ï¼Ÿ** å› ä¸ºä¼˜åŒ–å™¨æ‰§è¡Œæ¢¯åº¦ä¸‹é™ï¼ˆæœ€å°åŒ–ï¼‰ï¼Œè€Œæˆ‘ä»¬è¦æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°ï¼Œæ‰€ä»¥éœ€è¦æœ€å°åŒ–å…¶è´Ÿæ•°ã€‚

### 2. PPOå…³é”®ç»„ä»¶å®ç°

#### 2.1 ç­–ç•¥ç½‘ç»œï¼ˆPolicyNetï¼‰
```python
class PolicyNet(torch.nn.Module):
    """ç­–ç•¥ç½‘ç»œç±»ï¼Œç”¨äºè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        # å®šä¹‰ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚ï¼šçŠ¶æ€ç»´åº¦ -> éšè—å±‚ç»´åº¦
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        # å®šä¹‰ç¬¬äºŒå±‚å…¨è¿æ¥å±‚ï¼šéšè—å±‚ç»´åº¦ -> åŠ¨ä½œç»´åº¦
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        # é€šè¿‡ç¬¬ä¸€å±‚å¹¶åº”ç”¨ReLUæ¿€æ´»å‡½æ•°
        x = F.relu(self.fc1(x))
        # é€šè¿‡ç¬¬äºŒå±‚å¹¶åº”ç”¨softmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        return F.softmax(self.fc2(x), dim=1)
```

#### 2.2 ä»·å€¼ç½‘ç»œï¼ˆValueNetï¼‰
```python
class ValueNet(torch.nn.Module):
    """ä»·å€¼ç½‘ç»œç±»ï¼Œç”¨äºä¼°è®¡çŠ¶æ€ä»·å€¼å‡½æ•°"""
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        # å®šä¹‰ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚ï¼šçŠ¶æ€ç»´åº¦ -> éšè—å±‚ç»´åº¦
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        # å®šä¹‰ç¬¬äºŒå±‚å…¨è¿æ¥å±‚ï¼šéšè—å±‚ç»´åº¦ -> 1ï¼ˆä»·å€¼è¾“å‡ºï¼‰
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        # é€šè¿‡ç¬¬ä¸€å±‚å¹¶åº”ç”¨ReLUæ¿€æ´»å‡½æ•°
        x = F.relu(self.fc1(x))
        # é€šè¿‡ç¬¬äºŒå±‚è¾“å‡ºä»·å€¼ä¼°è®¡
        return self.fc2(x)
```

#### 2.3 å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
```python
def compute_advantage(self, gamma, lmbda, td_delta):
    """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰

    GAEå…¬å¼ï¼šA_t = Î´_t + Î³Î»A_{t+1}
    å…¶ä¸­Î´_tæ˜¯TDè¯¯å·®ï¼ŒÎ³æ˜¯æŠ˜æ‰£å› å­ï¼ŒÎ»æ˜¯GAEå‚æ•°
    """
    td_delta = td_delta.detach().numpy()
    advantage_list = []
    advantage = 0.0

    # ä»åå¾€å‰è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆé€†åºéå†TDè¯¯å·®ï¼‰
    for delta in td_delta[::-1]:
        advantage = gamma * lmbda * advantage + delta
        advantage_list.append(advantage)

    # ç”±äºæ˜¯é€†åºè®¡ç®—çš„ï¼Œéœ€è¦å°†åˆ—è¡¨åè½¬å›æ­£ç¡®çš„é¡ºåº
    advantage_list.reverse()
    return torch.tensor(advantage_list, dtype=torch.float)
```

### 3. PPOç®—æ³•æ ¸å¿ƒæ­¥éª¤

PPOç®—æ³•çš„è®­ç»ƒæµç¨‹åŒ…å«ä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š

#### 3.1 æ•°æ®æ”¶é›†é˜¶æ®µ
```python
# 1. ç¯å¢ƒäº¤äº’æ”¶é›†æ•°æ®
for i_episode in range(num_episodes):
    episode_return = 0
    transition_dict = {
        'states': [],      # å­˜å‚¨çŠ¶æ€åºåˆ—
        'actions': [],     # å­˜å‚¨åŠ¨ä½œåºåˆ—
        'next_states': [], # å­˜å‚¨ä¸‹ä¸€çŠ¶æ€åºåˆ—
        'rewards': [],     # å­˜å‚¨å¥–åŠ±åºåˆ—
        'dones': []        # å­˜å‚¨ç»“æŸæ ‡å¿—åºåˆ—
    }

    state, _ = env.reset()
    done = False

    while not done:
        # æ™ºèƒ½ä½“æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        action = agent.take_action(state)
        # ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œï¼Œè·å–åé¦ˆ
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        # å­˜å‚¨è½¬æ¢æ•°æ®
        transition_dict['states'].append(state)
        transition_dict['actions'].append(action)
        transition_dict['next_states'].append(next_state)
        transition_dict['rewards'].append(reward)
        transition_dict['dones'].append(done)

        state = next_state
        episode_return += reward
```

#### 3.2 ä¼˜åŠ¿å‡½æ•°è®¡ç®—
```python
# 2. è®¡ç®—TDç›®æ ‡å’Œä¼˜åŠ¿å‡½æ•°
# è®¡ç®—TDç›®æ ‡å€¼ï¼šr + Î³ * V(s') * (1 - done)
td_target = rewards + gamma * critic(next_states) * (1 - dones)
# è®¡ç®—TDè¯¯å·®ï¼šTDç›®æ ‡å€¼ - å½“å‰çŠ¶æ€ä»·å€¼ä¼°è®¡
td_delta = td_target - critic(states)
# ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•°
advantage = compute_advantage(gamma, lmbda, td_delta.cpu()).to(device)
```

#### 3.3 ç­–ç•¥æ›´æ–°ï¼ˆPPOæ ¸å¿ƒï¼‰
```python
# 3. è®¡ç®—æ—§ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡ï¼ˆå›ºå®šï¼Œä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼‰
old_log_probs = torch.log(actor(states).gather(1, actions)).detach()

# 4. å¤šè½®æ›´æ–°ï¼ˆé‡å¤ä½¿ç”¨åŒä¸€æ‰¹æ•°æ®ï¼‰
for _ in range(epochs):
    # è®¡ç®—å½“å‰ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
    log_probs = torch.log(actor(states).gather(1, actions))
    # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
    ratio = torch.exp(log_probs - old_log_probs)

    # è®¡ç®—ä¸¤ä¸ªä»£ç†ç›®æ ‡
    surr1 = ratio * advantage                    # æœªæˆªæ–­ç‰ˆæœ¬
    surr2 = torch.clamp(ratio, 1-eps, 1+eps) * advantage  # æˆªæ–­ç‰ˆæœ¬

    # PPOæŸå¤±å‡½æ•°ï¼šå–æœ€å°å€¼å¹¶è½¬æ¢ä¸ºæœ€å°åŒ–é—®é¢˜
    actor_loss = -torch.mean(torch.min(surr1, surr2))

    # ä»·å€¼ç½‘ç»œæŸå¤±
    critic_loss = torch.mean(F.mse_loss(critic(states), td_target.detach()))

    # æ¢¯åº¦æ›´æ–°
    actor_optimizer.zero_grad()
    critic_optimizer.zero_grad()
    actor_loss.backward()
    critic_loss.backward()
    actor_optimizer.step()
    critic_optimizer.step()
```

### 4. PPOå‚æ•°é…ç½®

#### 4.1 æ¨èå‚æ•°è®¾ç½®
```python
# ç½‘ç»œå‚æ•°
hidden_dim = 128        # éšè—å±‚ç»´åº¦

# è®­ç»ƒå‚æ•°
num_episodes = 500      # è®­ç»ƒå›åˆæ•°
gamma = 0.98           # æŠ˜æ‰£å› å­
lmbda = 0.95          # GAEå‚æ•°
epochs = 10           # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°

# PPOç‰¹æœ‰å‚æ•°
eps = 0.2             # æˆªæ–­å‚æ•°
actor_lr = 1e-3       # ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡
critic_lr = 1e-2      # ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡ï¼ˆé€šå¸¸æ¯”ç­–ç•¥ç½‘ç»œå¤§ï¼‰
```

#### 4.2 å‚æ•°è°ƒä¼˜æŒ‡å—

| å‚æ•° | ä½œç”¨ | è°ƒä¼˜å»ºè®® |
|------|------|----------|
| `eps` | æ§åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ | 0.1-0.3ï¼Œè¿‡å¤§ä¸ç¨³å®šï¼Œè¿‡å°å­¦ä¹ æ…¢ |
| `epochs` | æ•°æ®é‡ç”¨æ¬¡æ•° | 3-10ï¼Œè¿‡å¤šå¯èƒ½è¿‡æ‹Ÿåˆ |
| `lmbda` | GAEåå·®-æ–¹å·®æƒè¡¡ | 0.9-0.99ï¼Œæ¥è¿‘1ä½åå·®é«˜æ–¹å·® |
| `gamma` | æœªæ¥å¥–åŠ±é‡è¦æ€§ | 0.95-0.99ï¼Œé•¿æœŸä»»åŠ¡ç”¨æ›´å¤§å€¼ |
| `actor_lr` | ç­–ç•¥å­¦ä¹ é€Ÿåº¦ | 1e-4åˆ°1e-2ï¼Œé€šå¸¸æ¯”critic_lrå° |

### 5. ä½¿ç”¨è¯´æ˜

#### 5.1 ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch numpy matplotlib tqdm

# å®‰è£…gymnasiumåº“
pip install gymnasium[classic_control]
```

#### 5.2 è¿è¡ŒPPOç®—æ³•
```python
# ç›´æ¥è¿è¡Œ
python demo_13_PPO.py

# æˆ–è€…åœ¨ä»£ç ä¸­ä½¿ç”¨
import gymnasium as gym
from demo_13_PPO import PPO

# åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
env = gym.make('CartPole-v1')
agent = PPO(state_dim=4, hidden_dim=128, action_dim=2,
           actor_lr=1e-3, critic_lr=1e-2, lmbda=0.95,
           epochs=10, eps=0.2, gamma=0.98, device='cpu')

# è®­ç»ƒæ™ºèƒ½ä½“
return_list = []
for episode in range(500):
    # ... è®­ç»ƒå¾ªç¯
    pass
```

#### 5.3 è®­ç»ƒç»“æœå¯è§†åŒ–
```python
import matplotlib.pyplot as plt
import numpy as np

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
episodes_list = list(range(len(return_list)))

plt.figure(figsize=(12, 4))

# åŸå§‹å¥–åŠ±æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('PPO Training Progress')

# ç§»åŠ¨å¹³å‡æ›²çº¿
plt.subplot(1, 2, 2)
window_size = 10
if len(return_list) >= window_size:
    moving_avg = [np.mean(return_list[i:i+window_size])
                  for i in range(len(return_list)-window_size+1)]
    plt.plot(range(window_size-1, len(return_list)), moving_avg)
    plt.xlabel('Episodes')
    plt.ylabel('Moving Average Returns')
    plt.title('Smoothed Training Progress')

plt.tight_layout()
plt.show()
```

### 6. PPOä¼˜åŠ¿å’Œç‰¹ç‚¹

#### 6.1 ç®—æ³•ä¼˜åŠ¿
1. **ç®€å•æ˜“å®ç°**ï¼šç›¸æ¯”TRPOï¼ŒPPOä¸éœ€è¦å¤æ‚çš„å…±è½­æ¢¯åº¦æ³•å’Œçº¿æ€§æœç´¢
2. **è®¡ç®—é«˜æ•ˆ**ï¼šåªéœ€è¦ä¸€é˜¶æ¢¯åº¦ä¿¡æ¯ï¼Œè®¡ç®—å¼€é”€å°
3. **ç¨³å®šæ€§å¥½**ï¼šé€šè¿‡æˆªæ–­æœºåˆ¶é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ï¼Œè®­ç»ƒç¨³å®š
4. **æ ·æœ¬æ•ˆç‡é«˜**ï¼šé€šè¿‡å¤šè½®æ›´æ–°é‡å¤ä½¿ç”¨æ•°æ®ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡
5. **è¶…å‚æ•°é²æ£’**ï¼šå¯¹è¶…å‚æ•°è®¾ç½®ç›¸å¯¹ä¸æ•æ„Ÿï¼Œæ˜“äºè°ƒä¼˜

#### 6.2 å®ç°ç‰¹ç‚¹
1. **å®Œæ•´æ³¨é‡Š**ï¼šæ¯è¡Œå…³é”®ä»£ç éƒ½æœ‰è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šå’Œè§£é‡Š
2. **è‡ªåŒ…å«å®ç°**ï¼šä¸ä¾èµ–å¤–éƒ¨å·¥å…·å‡½æ•°ï¼Œæ‰€æœ‰åŠŸèƒ½éƒ½åœ¨ä¸»æ–‡ä»¶ä¸­å®ç°
3. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„ç±»ç»“æ„ï¼Œä¾¿äºç†è§£å’Œä¿®æ”¹
4. **å…¼å®¹æ€§å¥½**ï¼šä½¿ç”¨æœ€æ–°çš„gymnasiumåº“ï¼Œæ”¯æŒç°ä»£å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ

#### 6.3 æ€§èƒ½è¡¨ç°

**CartPole-v1ç¯å¢ƒæµ‹è¯•ç»“æœï¼š**
- **æ”¶æ•›é€Ÿåº¦**ï¼šé€šå¸¸åœ¨100-200ä¸ªepisodeså†…è¾¾åˆ°æ»¡åˆ†ï¼ˆ500åˆ†ï¼‰
- **è®­ç»ƒç¨³å®šæ€§**ï¼šè®­ç»ƒè¿‡ç¨‹å¹³ç¨³ï¼Œå¾ˆå°‘å‡ºç°æ€§èƒ½å¤§å¹…æ³¢åŠ¨
- **æœ€ç»ˆæ€§èƒ½**ï¼šèƒ½å¤Ÿç¨³å®šè¾¾åˆ°æœ€å¤§å¥–åŠ±500åˆ†
- **è®¡ç®—æ•ˆç‡**ï¼šå•æ¬¡è®­ç»ƒçº¦éœ€1-2åˆ†é’Ÿï¼ˆCPUï¼‰

### 7. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 7.1 è®­ç»ƒä¸ç¨³å®š
**ç—‡çŠ¶**ï¼šå¥–åŠ±æ›²çº¿å‰§çƒˆæ³¢åŠ¨ï¼Œæ€§èƒ½æ—¶å¥½æ—¶å
**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å‡å°æˆªæ–­å‚æ•°
eps = 0.1  # ä»0.2å‡å°åˆ°0.1

# 2. å‡å°å­¦ä¹ ç‡
actor_lr = 5e-4  # ä»1e-3å‡å°åˆ°5e-4
critic_lr = 5e-3  # ä»1e-2å‡å°åˆ°5e-3

# 3. å‡å°‘æ›´æ–°è½®æ•°
epochs = 5  # ä»10å‡å°åˆ°5
```

#### 7.2 å­¦ä¹ é€Ÿåº¦æ…¢
**ç—‡çŠ¶**ï¼šè®­ç»ƒå¾ˆå¤šepisodesåæ€§èƒ½ä»ç„¶å¾ˆå·®
**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å¢åŠ ç½‘ç»œå®¹é‡
hidden_dim = 256  # ä»128å¢åŠ åˆ°256

# 2. è°ƒæ•´GAEå‚æ•°
lmbda = 0.99  # ä»0.95å¢åŠ åˆ°0.99

# 3. å¢åŠ æ›´æ–°è½®æ•°
epochs = 15  # ä»10å¢åŠ åˆ°15
```

#### 7.3 è¿‡æ‹Ÿåˆé—®é¢˜
**ç—‡çŠ¶**ï¼šè®­ç»ƒåæœŸæ€§èƒ½ä¸‹é™ï¼ŒéªŒè¯æ€§èƒ½å·®
**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å‡å°‘æ›´æ–°è½®æ•°
epochs = 3  # ä»10å‡å°åˆ°3

# 2. å¢åŠ æ­£åˆ™åŒ–
# åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ ç†µæ­£åˆ™åŒ–
entropy_coef = 0.01
entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
actor_loss = actor_loss - entropy_coef * torch.mean(entropy)
```

### 8. ä¸å…¶ä»–ç®—æ³•çš„æ¯”è¾ƒ

#### 8.1 PPO vs TRPO
| æ–¹é¢ | PPO | TRPO |
|------|-----|------|
| **ç†è®ºä¿è¯** | å¯å‘å¼æˆªæ–­ | ä¸¥æ ¼çš„KLçº¦æŸ |
| **å®ç°éš¾åº¦** | ç®€å• | å¤æ‚ |
| **è®¡ç®—å¼€é”€** | ä½ | é«˜ |
| **è°ƒå‚éš¾åº¦** | å®¹æ˜“ | å›°éš¾ |
| **å®é™…æ€§èƒ½** | ä¼˜ç§€ | ä¼˜ç§€ |

#### 8.2 PPO vs A3C
| æ–¹é¢ | PPO | A3C |
|------|-----|------|
| **å¹¶è¡Œæ€§** | å¯é€‰ | å¿…éœ€ |
| **æ ·æœ¬æ•ˆç‡** | é«˜ï¼ˆæ•°æ®é‡ç”¨ï¼‰ | ä½ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰ |
| **ç¨³å®šæ€§** | é«˜ | ä¸­ç­‰ |
| **å®ç°å¤æ‚åº¦** | ä¸­ç­‰ | é«˜ |

#### 8.3 PPO vs DQN
| æ–¹é¢ | PPO | DQN |
|------|-----|------|
| **åŠ¨ä½œç©ºé—´** | è¿ç»­+ç¦»æ•£ | ä»…ç¦»æ•£ |
| **ç­–ç•¥ç±»å‹** | éšæœºç­–ç•¥ | ç¡®å®šæ€§ç­–ç•¥ |
| **æ¢ç´¢æœºåˆ¶** | å†…ç½®éšæœºæ€§ | Îµ-è´ªå©ª |
| **æ ·æœ¬æ•ˆç‡** | ä¸­ç­‰ | é«˜ï¼ˆç»éªŒå›æ”¾ï¼‰ |

### 9. æ‰©å±•å’Œæ”¹è¿›æ–¹å‘

#### 9.1 å¯èƒ½çš„æ”¹è¿›
1. **è‡ªé€‚åº”æˆªæ–­å‚æ•°**ï¼šæ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´epså€¼
2. **å¤šç¯å¢ƒå¹¶è¡Œ**ï¼šä½¿ç”¨å¤šä¸ªç¯å¢ƒå®ä¾‹å¹¶è¡Œæ”¶é›†æ•°æ®
3. **ä¼˜å…ˆç»éªŒå›æ”¾**ï¼šç»“åˆé‡è¦æ€§é‡‡æ ·æ”¹è¿›æ•°æ®åˆ©ç”¨
4. **è¿ç»­åŠ¨ä½œæ‰©å±•**ï¼šæ‰©å±•åˆ°è¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆé«˜æ–¯ç­–ç•¥ï¼‰

#### 9.2 é«˜çº§æŠ€å·§
```python
# 1. æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=0.5)

# 2. å­¦ä¹ ç‡è°ƒåº¦
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)

# 3. æ—©åœæœºåˆ¶
if np.mean(return_list[-10:]) > target_score:
    print("ä»»åŠ¡å®Œæˆï¼Œæå‰åœæ­¢è®­ç»ƒ")
    break
```

## TRPOç®—æ³•è¯¦ç»†è¯´æ˜

TRPOï¼ˆTrust Region Policy Optimizationï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦æ¥ç¡®ä¿è®­ç»ƒçš„ç¨³å®šæ€§å’Œå•è°ƒæ”¹è¿›ã€‚æœ¬é¡¹ç›®æä¾›äº†ä¸¤ä¸ªç‰ˆæœ¬çš„å®ç°ï¼š

### ğŸ“‹ å®ç°ç‰ˆæœ¬å¯¹æ¯”

| ç‰¹æ€§ | demo_11_TRPO_CartPole.py | demo_12_TRPO_pendulum.py |
|------|-------------------------|--------------------------|
| **åŠ¨ä½œç©ºé—´** | ç¦»æ•£åŠ¨ä½œï¼ˆCategoricalåˆ†å¸ƒï¼‰ | è¿ç»­åŠ¨ä½œï¼ˆNormalåˆ†å¸ƒï¼‰ |
| **ç¯å¢ƒ** | CartPole-v1 | Pendulum-v1 |
| **ç­–ç•¥ç½‘ç»œè¾“å‡º** | åŠ¨ä½œæ¦‚ç‡å‘é‡ | é«˜æ–¯åˆ†å¸ƒå‚æ•°ï¼ˆÎ¼, Ïƒï¼‰ |
| **åŠ¨ä½œé‡‡æ ·** | ç±»åˆ«åˆ†å¸ƒé‡‡æ · | æ­£æ€åˆ†å¸ƒé‡‡æ · |
| **KLæ•£åº¦è®¡ç®—** | ç¦»æ•£åˆ†å¸ƒKLæ•£åº¦ | è¿ç»­åˆ†å¸ƒKLæ•£åº¦ |
| **é€‚ç”¨åœºæ™¯** | æ¸¸æˆã€å¯¼èˆªç­‰ç¦»æ•£å†³ç­– | æœºå™¨äººæ§åˆ¶ã€è¿ç»­æ§åˆ¶ |

### 1. æ•°å­¦åŸç†

TRPOç®—æ³•çš„æ ¸å¿ƒæ˜¯æ±‚è§£å¦‚ä¸‹çº¦æŸä¼˜åŒ–é—®é¢˜ï¼š

```
æœ€å¤§åŒ–ï¼šE[Ï€_new(a|s)/Ï€_old(a|s) * A(s,a)]
çº¦æŸæ¡ä»¶ï¼šE[KL(Ï€_old(Â·|s) || Ï€_new(Â·|s))] â‰¤ Î´
```

å…¶ä¸­ï¼š
- Ï€_new å’Œ Ï€_old åˆ†åˆ«æ˜¯æ–°æ—§ç­–ç•¥
- A(s,a) æ˜¯ä¼˜åŠ¿å‡½æ•°
- KL æ˜¯KLæ•£åº¦ï¼Œç”¨äºåº¦é‡ç­–ç•¥æ›´æ–°çš„å¹…åº¦
- Î´ æ˜¯KLæ•£åº¦çº¦æŸé˜ˆå€¼

#### 1.1 è¿ç»­åŠ¨ä½œç©ºé—´çš„ç‰¹æ®Šå¤„ç†

å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œç­–ç•¥ç½‘ç»œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ï¼š
```
Ï€(a|s) = N(Î¼(s), Ïƒ(s))
```
å…¶ä¸­Î¼(s)å’ŒÏƒ(s)åˆ†åˆ«æ˜¯çŠ¶æ€sä¸‹çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚

### 2. å…³é”®ç»„ä»¶å®ç°

#### 2.1 å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
```python
def compute_advantage(gamma, lmbda, td_delta):
    """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡
    A(s,a) = Î´_t + (Î³Î»)Î´_{t+1} + ... + (Î³Î»)^{T-t-1}Î´_{T-1}
    å…¶ä¸­ Î´_t æ˜¯TDè¯¯å·®
    """
    advantage = 0.0
    advantage_list = []
    for delta in td_delta[::-1]:  # ä»åå‘å‰è®¡ç®—
        advantage = gamma * lmbda * advantage + delta
        advantage_list.append(advantage)
    advantage_list.reverse()
    return torch.tensor(advantage_list, dtype=torch.float)
```

#### 2.2 ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰

**ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆCartPoleï¼‰ï¼š**
```python
class PolicyNet(torch.nn.Module):
    """å°†çŠ¶æ€æ˜ å°„ä¸ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒçš„ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)  # è¾“å‡ºåŠ¨ä½œæ¦‚ç‡
```

**è¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆPendulumï¼‰ï¼š**
```python
class PolicyNetContinuous(torch.nn.Module):
    """è¿ç»­åŠ¨ä½œç©ºé—´çš„ç­–ç•¥ç½‘ç»œï¼Œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒå‚æ•°"""
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNetContinuous, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)    # å‡å€¼
        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)   # æ ‡å‡†å·®

    def forward(self, x):
        x = F.relu(self.fc1(x))
        mu = 2.0 * torch.tanh(self.fc_mu(x))    # é™åˆ¶åŠ¨ä½œèŒƒå›´
        std = F.softplus(self.fc_std(x))        # ç¡®ä¿æ ‡å‡†å·®ä¸ºæ­£
        return mu, std
```

#### 2.3 ä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰
```python
class ValueNet(torch.nn.Module):
    """è¯„ä¼°çŠ¶æ€ä»·å€¼çš„ç½‘ç»œï¼ˆä¸¤ä¸ªç‰ˆæœ¬é€šç”¨ï¼‰"""
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

### 3. ç®—æ³•æ ¸å¿ƒæ­¥éª¤

TRPOç®—æ³•çš„æ‰§è¡Œæµç¨‹åŒ…å«ä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š

1. **æ”¶é›†è½¨è¿¹æ•°æ®**
   - ä½¿ç”¨å½“å‰ç­–ç•¥Ï€_oldä¸ç¯å¢ƒäº¤äº’
   - å­˜å‚¨çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€çŠ¶æ€ç­‰ä¿¡æ¯
   - æ”¶é›†å®Œæ•´çš„episodeæ•°æ®

2. **è®¡ç®—ä¼˜åŠ¿ä¼°è®¡**
   - ä½¿ç”¨GAEæ–¹æ³•è®¡ç®—ä¼˜åŠ¿å‡½æ•°å€¼A(s,a)
   - ç»“åˆæ—¶åºå·®åˆ†è¯¯å·®å’Œå¤šæ­¥å›æŠ¥
   - å¹³è¡¡åå·®å’Œæ–¹å·®

3. **ç­–ç•¥æ›´æ–°ï¼ˆTRPOæ ¸å¿ƒï¼‰**
   - **æ­¥éª¤3.1**: è®¡ç®—æ›¿ä»£ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦g
   - **æ­¥éª¤3.2**: ä½¿ç”¨å…±è½­æ¢¯åº¦æ³•æ±‚è§£Hx = gï¼Œå¾—åˆ°è‡ªç„¶æ¢¯åº¦æ–¹å‘x
   - **æ­¥éª¤3.3**: è®¡ç®—æœ€å¤§æ­¥é•¿ï¼šmax_coef = âˆš(2Î´/(x^T H x))
   - **æ­¥éª¤3.4**: æ‰§è¡Œçº¿æ€§æœç´¢ï¼Œç¡®ä¿æ»¡è¶³KLçº¦æŸå’Œæ€§èƒ½æ”¹è¿›
   - **æ­¥éª¤3.5**: æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°

4. **ä»·å€¼å‡½æ•°æ›´æ–°**
   - ä½¿ç”¨æ—¶åºå·®åˆ†ç›®æ ‡æ›´æ–°ä»·å€¼ç½‘ç»œ
   - æœ€å°åŒ–ä»·å€¼ä¼°è®¡çš„å‡æ–¹è¯¯å·®
   - ä¸ºä¸‹ä¸€è½®ä¼˜åŠ¿ä¼°è®¡æä¾›åŸºå‡†

#### 3.1 å…±è½­æ¢¯åº¦æ³•è¯¦è§£

å…±è½­æ¢¯åº¦æ³•ç”¨äºé«˜æ•ˆæ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„Hx = gï¼š
```python
def conjugate_gradient(self, grad, states, old_action_dists):
    x = torch.zeros_like(grad)  # åˆå§‹è§£
    r = grad.clone()            # åˆå§‹æ®‹å·®
    p = grad.clone()            # åˆå§‹æœç´¢æ–¹å‘

    for i in range(10):  # æœ€å¤š10æ¬¡è¿­ä»£
        Hp = self.hessian_matrix_vector_product(states, old_action_dists, p)
        alpha = torch.dot(r, r) / torch.dot(p, Hp)
        x += alpha * p
        r -= alpha * Hp

        if torch.dot(r, r) < 1e-10:  # æ”¶æ•›æ£€æŸ¥
            break

        beta = torch.dot(r, r) / torch.dot(r_old, r_old)
        p = r + beta * p

    return x
```

#### 3.2 çº¿æ€§æœç´¢æœºåˆ¶

çº¿æ€§æœç´¢ç¡®ä¿ç­–ç•¥æ›´æ–°æ—¢æ”¹å–„æ€§èƒ½åˆæ»¡è¶³çº¦æŸï¼š
```python
def line_search(self, states, actions, advantage, old_log_probs,
                old_action_dists, max_vec):
    for i in range(15):  # æœ€å¤š15æ¬¡å°è¯•
        coef = self.alpha ** i  # æ­¥é•¿è¡°å‡
        new_para = old_para + coef * max_vec

        # æ£€æŸ¥çº¦æŸæ¡ä»¶
        if new_obj > old_obj and kl_div < self.kl_constraint:
            return new_para  # æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„æ›´æ–°

    return old_para  # ä¿å®ˆç­–ç•¥ï¼šä¸æ›´æ–°
```

### 4. å‚æ•°é…ç½®

#### 4.1 CartPoleç¯å¢ƒï¼ˆç¦»æ•£åŠ¨ä½œï¼‰
```python
# ç½‘ç»œå‚æ•°
hidden_dim = 128        # éšè—å±‚ç»´åº¦

# è®­ç»ƒå‚æ•°
num_episodes = 500      # è®­ç»ƒå›åˆæ•°
gamma = 0.98           # æŠ˜æ‰£å› å­
lmbda = 0.95          # GAEå‚æ•°
critic_lr = 1e-2       # è¯„è®ºå®¶å­¦ä¹ ç‡

# TRPOç‰¹æœ‰å‚æ•°
kl_constraint = 0.0005  # KLæ•£åº¦çº¦æŸ
alpha = 0.5            # çº¿æ€§æœç´¢æ­¥é•¿è¡°å‡
```

#### 4.2 Pendulumç¯å¢ƒï¼ˆè¿ç»­åŠ¨ä½œï¼‰
```python
# ç½‘ç»œå‚æ•°
hidden_dim = 128        # éšè—å±‚ç»´åº¦

# è®­ç»ƒå‚æ•°
num_episodes = 2000     # è®­ç»ƒå›åˆæ•°ï¼ˆè¿ç»­æ§åˆ¶éœ€è¦æ›´å¤šè®­ç»ƒï¼‰
gamma = 0.9            # æŠ˜æ‰£å› å­
lmbda = 0.9           # GAEå‚æ•°
critic_lr = 1e-2       # è¯„è®ºå®¶å­¦ä¹ ç‡

# TRPOç‰¹æœ‰å‚æ•°
kl_constraint = 0.00005 # KLæ•£åº¦çº¦æŸï¼ˆæ›´ä¸¥æ ¼ï¼‰
alpha = 0.5            # çº¿æ€§æœç´¢æ­¥é•¿è¡°å‡
```

#### 4.3 å‚æ•°è°ƒä¼˜å»ºè®®

| å‚æ•° | ä½œç”¨ | è°ƒä¼˜å»ºè®® |
|------|------|----------|
| `kl_constraint` | æ§åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ | è¿‡å¤§â†’ä¸ç¨³å®šï¼›è¿‡å°â†’å­¦ä¹ æ…¢ |
| `lmbda` | GAEåå·®-æ–¹å·®æƒè¡¡ | æ¥è¿‘1â†’ä½åå·®é«˜æ–¹å·®ï¼›æ¥è¿‘0â†’é«˜åå·®ä½æ–¹å·® |
| `gamma` | æœªæ¥å¥–åŠ±é‡è¦æ€§ | é•¿æœŸä»»åŠ¡ç”¨0.99ï¼›çŸ­æœŸä»»åŠ¡ç”¨0.9 |
| `alpha` | çº¿æ€§æœç´¢æ¿€è¿›ç¨‹åº¦ | 0.5æ˜¯ç»éªŒå€¼ï¼Œå¯å°è¯•0.3-0.8 |

### 5. ä½¿ç”¨è¯´æ˜

#### 5.1 ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch numpy matplotlib tqdm

# å®‰è£…gymnasiumåº“
pip install gymnasium[classic_control]
```

#### 5.2 è¿è¡Œç¦»æ•£åŠ¨ä½œç‰ˆæœ¬ï¼ˆCartPoleï¼‰
```python
# è¿è¡Œdemo_11_TRPO_CartPole.py
python demo_11_TRPO_CartPole.py

# åˆ›å»ºæ™ºèƒ½ä½“ç¤ºä¾‹
env = gym.make('CartPole-v1')
agent = TRPO(hidden_dim, env.observation_space, env.action_space,
            lmbda, kl_constraint, alpha, critic_lr, gamma, device)
return_list = train_on_policy_agent(env, agent, num_episodes)
```

#### 5.3 è¿è¡Œè¿ç»­åŠ¨ä½œç‰ˆæœ¬ï¼ˆPendulumï¼‰
```python
# è¿è¡Œdemo_12_TRPO_pendulum.py
python demo_12_TRPO_pendulum.py

# åˆ›å»ºæ™ºèƒ½ä½“ç¤ºä¾‹
env = gym.make('Pendulum-v1')
agent = TRPOContinuous(hidden_dim, env.observation_space, env.action_space,
                      lmbda, kl_constraint, alpha, critic_lr, gamma, device)
return_list = train_on_policy_agent(env, agent, num_episodes)
```

#### 5.4 ç»“æœå¯è§†åŒ–
```python
# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
episodes_list = list(range(len(return_list)))

# åŸå§‹æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(episodes_list, return_list)
plt.title('Raw Training Progress')

# ç§»åŠ¨å¹³å‡æ›²çº¿
plt.subplot(1, 2, 2)
mv_return = moving_average(return_list, 9)
plt.plot(episodes_list, mv_return)
plt.title('Smoothed Training Progress')

plt.show()
```

### 6. ä¼˜åŠ¿å’Œç‰¹ç‚¹

#### 6.1 ç®—æ³•ä¼˜åŠ¿
1. **ç¨³å®šæ€§**ï¼šé€šè¿‡KLæ•£åº¦çº¦æŸç¡®ä¿ç­–ç•¥æ›´æ–°çš„ä¿å®ˆæ€§ï¼Œé¿å…ç­–ç•¥å´©æºƒ
2. **å¯é æ€§**ï¼šç†è®ºä¸Šä¿è¯å•è°ƒç­–ç•¥æ”¹è¿›ï¼Œæ¯æ¬¡æ›´æ–°éƒ½ä¸ä¼šå˜å·®
3. **é€‚ç”¨æ€§**ï¼šåŒæ—¶æ”¯æŒç¦»æ•£å’Œè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œåº”ç”¨èŒƒå›´å¹¿æ³›
4. **æ•ˆç‡**ï¼šä½¿ç”¨å…±è½­æ¢¯åº¦æ³•é«˜æ•ˆæ±‚è§£äºŒé˜¶ä¼˜åŒ–é—®é¢˜ï¼Œé¿å…ç›´æ¥è®¡ç®—HessiançŸ©é˜µ

#### 6.2 å®ç°ç‰¹ç‚¹
1. **å®Œæ•´æ³¨é‡Š**ï¼šæ¯è¡Œå…³é”®ä»£ç éƒ½æœ‰è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
2. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„ç±»ç»“æ„ï¼Œä¾¿äºç†è§£å’Œä¿®æ”¹
3. **å…¼å®¹æ€§å¥½**ï¼šæ”¯æŒæ–°æ—§ç‰ˆæœ¬çš„gym/gymnasium
4. **å¯è§†åŒ–ä¸°å¯Œ**ï¼šæä¾›å¤šç§è®­ç»ƒæ›²çº¿å’Œæ€§èƒ½åˆ†æå›¾è¡¨

#### 6.3 æ€§èƒ½è¡¨ç°

**CartPoleç¯å¢ƒï¼ˆç¦»æ•£åŠ¨ä½œï¼‰ï¼š**
- é€šå¸¸åœ¨100-200ä¸ªepisodeså†…è¾¾åˆ°æ»¡åˆ†ï¼ˆ500åˆ†ï¼‰
- è®­ç»ƒç¨³å®šï¼Œå¾ˆå°‘å‡ºç°æ€§èƒ½å€’é€€
- é€‚åˆä½œä¸ºTRPOç®—æ³•çš„å…¥é—¨ç¤ºä¾‹

**Pendulumç¯å¢ƒï¼ˆè¿ç»­åŠ¨ä½œï¼‰ï¼š**
- ä»åˆå§‹-1400å·¦å³é€æ­¥æ”¹å–„åˆ°-200ä»¥å†…
- éœ€è¦æ›´å¤šè®­ç»ƒepisodesï¼ˆ2000+ï¼‰
- å±•ç¤ºäº†TRPOåœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­çš„èƒ½åŠ›

### 7. æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ

#### 7.1 å…³é”®æ³¨æ„äº‹é¡¹
1. **KLæ•£åº¦çº¦æŸ**ï¼šè¿™æ˜¯æœ€å…³é”®çš„è¶…å‚æ•°
   - è¿‡å¤§ï¼ˆ>0.01ï¼‰ï¼šå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œç­–ç•¥æ›´æ–°è¿‡äºæ¿€è¿›
   - è¿‡å°ï¼ˆ<0.0001ï¼‰ï¼šå­¦ä¹ é€Ÿåº¦æ…¢ï¼Œç­–ç•¥æ›´æ–°è¿‡äºä¿å®ˆ
   - æ¨èå€¼ï¼šç¦»æ•£åŠ¨ä½œ0.0005ï¼Œè¿ç»­åŠ¨ä½œ0.00005

2. **æ‰¹é‡æ•°æ®å¤§å°**ï¼šéœ€è¦è¶³å¤Ÿçš„æ•°æ®æ¥å‡†ç¡®ä¼°è®¡KLæ•£åº¦
   - æ¯ä¸ªepisodeçš„æ•°æ®éƒ½å¾ˆé‡è¦
   - ä¸å»ºè®®ä½¿ç”¨mini-batchï¼Œè€Œæ˜¯ä½¿ç”¨å®Œæ•´episodeæ•°æ®

3. **è®¡ç®—å¤æ‚åº¦**ï¼šç›¸æ¯”ç®€å•çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•æ›´å¤æ‚
   - å…±è½­æ¢¯åº¦æ³•éœ€è¦å¤šæ¬¡Hessian-vectorä¹˜ç§¯è®¡ç®—
   - çº¿æ€§æœç´¢éœ€è¦å¤šæ¬¡å‰å‘ä¼ æ’­
   - å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿ

4. **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šå¯¹å‚æ•°è®¾ç½®è¾ƒä¸ºæ•æ„Ÿ
   - å»ºè®®ä»æ¨èå€¼å¼€å§‹ï¼Œé€æ­¥å¾®è°ƒ
   - ä¸åŒç¯å¢ƒå¯èƒ½éœ€è¦ä¸åŒçš„å‚æ•°è®¾ç½®

#### 7.2 è°ƒè¯•å’Œä¼˜åŒ–å»ºè®®

**è®­ç»ƒä¸ç¨³å®šçš„è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. å‡å°KLæ•£åº¦çº¦æŸ
kl_constraint = 0.0001  # ä»0.0005å‡å°åˆ°0.0001

# 2. è°ƒæ•´GAEå‚æ•°
lmbda = 0.8  # ä»0.95å‡å°åˆ°0.8ï¼Œé™ä½æ–¹å·®

# 3. å¢åŠ é˜»å°¼ç³»æ•°
damping = 0.1  # åœ¨hessian_matrix_vector_productä¸­
```

**è®­ç»ƒé€Ÿåº¦æ…¢çš„è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. å‡å°ç½‘ç»œè§„æ¨¡
hidden_dim = 64  # ä»128å‡å°åˆ°64

# 2. å‡å°‘å…±è½­æ¢¯åº¦è¿­ä»£æ¬¡æ•°
cg_iters = 5  # ä»10å‡å°åˆ°5

# 3. å‡å°‘çº¿æ€§æœç´¢æ¬¡æ•°
max_backtracks = 10  # ä»15å‡å°åˆ°10
```

**æ€§èƒ½ä¸ç†æƒ³çš„è§£å†³æ–¹æ¡ˆï¼š**
```python
# 1. å¢åŠ è®­ç»ƒepisodes
num_episodes = 1000  # é€‚å½“å¢åŠ 

# 2. è°ƒæ•´å¥–åŠ±ç¼©æ”¾ï¼ˆç‰¹åˆ«æ˜¯Pendulumï¼‰
rewards = (rewards + 8.0) / 8.0  # å¥–åŠ±æ ‡å‡†åŒ–

# 3. è°ƒæ•´ç½‘ç»œåˆå§‹åŒ–
torch.nn.init.orthogonal_(layer.weight, gain=0.01)
```

### 8. å®éªŒç»“æœå±•ç¤º

#### 8.1 CartPoleç¯å¢ƒè®­ç»ƒç»“æœ
```
ç¯å¢ƒ: CartPole-v1
è®­ç»ƒepisodes: 500
æœ€ç»ˆæ€§èƒ½: 500.0 (æ»¡åˆ†)
æ”¶æ•›é€Ÿåº¦: ~150 episodes
ç¨³å®šæ€§: é«˜ï¼Œå¾ˆå°‘å‡ºç°æ€§èƒ½å€’é€€
```

#### 8.2 Pendulumç¯å¢ƒè®­ç»ƒç»“æœ
```
ç¯å¢ƒ: Pendulum-v1
è®­ç»ƒepisodes: 2000
åˆå§‹æ€§èƒ½: -1453.235
æœ€ç»ˆæ€§èƒ½: -474.927
æ”¹å–„å¹…åº¦: ~67%
æ”¶æ•›ç‰¹ç‚¹: é€æ­¥ç¨³å®šæ”¹å–„ï¼Œæ— æ˜æ˜¾éœ‡è¡
```

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### åŸºç¡€ä¾èµ–
- **Python**: 3.7+ ï¼ˆæ¨è3.8+ï¼‰
- **PyTorch**: 1.8+ ï¼ˆæ¨èæœ€æ–°ç‰ˆæœ¬ï¼‰
- **NumPy**: 1.19+
- **Matplotlib**: 3.3+
- **tqdm**: 4.60+ ï¼ˆè¿›åº¦æ¡æ˜¾ç¤ºï¼‰

### å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
```bash
# ç»Ÿä¸€ä½¿ç”¨gymnasiumåº“
pip install gymnasium[classic_control]
```

### å¯é€‰åŠ é€Ÿ
- **CUDA**: æ”¯æŒGPUåŠ é€Ÿï¼ˆæ¨èï¼‰
- **cuDNN**: æ·±åº¦å­¦ä¹ åŠ é€Ÿåº“

### å®Œæ•´å®‰è£…å‘½ä»¤
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n rl_env python=3.8
conda activate rl_env

# å®‰è£…PyTorchï¼ˆæ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
pip install torch torchvision torchaudio

# å®‰è£…å…¶ä»–ä¾èµ–
pip install gymnasium[classic_control] numpy matplotlib tqdm

# éªŒè¯å®‰è£…
python -c "import torch; import gymnasium; print('å®‰è£…æˆåŠŸï¼')"
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **PPOåŸå§‹è®ºæ–‡**: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

2. **TRPOåŸå§‹è®ºæ–‡**: Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. In International conference on machine learning (pp. 1889-1897).

3. **GAEè®ºæ–‡**: Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.

4. **ç­–ç•¥æ¢¯åº¦ç»¼è¿°**: Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12.

5. **OpenAI PPOå®ç°**: Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., ... & Wu, J. (2017). OpenAI baselines. GitHub repository.

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### è´¡çŒ®æ–¹å¼
1. Forkæœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

### ä»£ç è§„èŒƒ
- ä¿æŒè¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
- éµå¾ªGoogle Pythonä»£ç é£æ ¼
- æ·»åŠ é€‚å½“çš„ç±»å‹æç¤º
- ç¡®ä¿ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ

---

**é¡¹ç›®ç»´æŠ¤è€…**: [POPO]
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ
**è®¸å¯è¯**: MIT License
